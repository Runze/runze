---
author: Runze
date: 2014-11-15 22:21:43+00:00
slug: notes-to-machine-learning
title: Notes to machine learning
wordpress_id: 334
output:
  blogdown::html_page:
    toc: true
categories:
- Data Analysis
tags:
- Machine learning
---


<div id="TOC">
<ul>
<li><a href="#data-transformation">Data transformation</a></li>
<li><a href="#resampling-techniques">Resampling techniques</a></li>
<li><a href="#regression-models">Regression models</a></li>
<li><a href="#smoothing">Smoothing</a></li>
<li><a href="#neural-networks">Neural networks</a></li>
<li><a href="#support-vector-machines">Support vector machines</a></li>
<li><a href="#k-nearest-neighbors">K-nearest neighbors</a></li>
<li><a href="#trees">Trees</a></li>
<li><a href="#random-forests">Random forests</a></li>
<li><a href="#gradient-boosting-trees">Gradient boosting trees</a></li>
<li><a href="#cubist">Cubist</a></li>
<li><a href="#measuring-performace-in-classification-models">Measuring performace in classification models</a></li>
<li><a href="#linear-classificatoin-models">Linear classificatoin models</a></li>
<li><a href="#latent-dirichlet-allocation">Latent Dirichlet allocation</a></li>
</ul>
</div>

<p>These are the notes I took while reading <a href="http://www-bcf.usc.edu/~gareth/ISL/">An Introduction to Statistical Learning</a> and <a href="http://appliedpredictivemodeling.com/">Applied Predictive Modeling</a>. Some of the notes also came from other sources, but the majority of them are from these two books.</p>
<div id="data-transformation" class="section level3">
<h3>Data transformation</h3>
<div id="centering-and-scaling" class="section level4">
<h4>Centering and scaling</h4>
</div>
<div id="skewness" class="section level4">
<h4>Skewness</h4>
<ul>
<li>Skew: the dimensionless version of the 3rd moment about the mean. <code>$$m_3 = \frac{\sum(y - \bar{y})^3}{n}$$</code> <span class="math display">\[s_3 = sd(y)^3 = (\sqrt{s^2})^3\]</span> <span class="math display">\[skew = \frac{m_3}{s_3}\]</span> To test whether a particular value of skew is significantly different from 0, divide the estimate by its approximate standard error: <span class="math display">\[se_{skew} = \sqrt{\frac{6}{n}}\]</span></li>
<li><p>Kurtosis: the dimensionless version of the 4th moment about the mean. <span class="math display">\[m_4 = \frac{\sum(y - \bar{y})^4}{n}\]</span> <span class="math display">\[s_4 = sd(y)^4 = (s^2)^2\]</span> <span class="math display">\[kurtosis = \frac{m_4}{s_4} - 3\]</span> Its approximate standard error is: <span class="math display">\[se_{kurtosis} = \sqrt{\frac{24}{n}}\]</span></p></li>
<li><p>Box-cox transformation: <span class="math display">\[ x^* = \left\{ 
  \begin{array}{l l}
\frac{x^\lambda - 1}{\lambda} &amp; \quad \text{if $\lambda \neq$ 0}\\
log(x) &amp; \quad \text{if $\lambda$ = 0}
  \end{array} \right.\]</span></p></li>
</ul>
</div>
<div id="spatial-sign-transformation-to-remove-outliers" class="section level4">
<h4>Spatial sign transformation to remove outliers</h4>
<p><span class="math display">\[x^*_{ij} = \frac{x_{ij}}{\sum_{j=1}^{P}x^2_{ij}}\]</span> Since the denominator is intended to measure the squared distance to the center of the predictor’s distribution, it is important to <em>center and scale</em> the predictor data prior to using this transformation.</p>
<p>Also note that, unlike centering or scaling, this manipulation of the predictors transforms them as a group. Removing predictor variables after applying the spatial sign transformation may be problematic.</p>
</div>
<div id="feature-extraction" class="section level4">
<h4>Feature extraction</h4>
<p>PCA is a commonly used data reduction technique. The <em>j</em>th PC can be written as: <span class="math display">\[PC_j = (a_{j1} \times Predictor 1) + (a_{j2} \times Predictor 2) + \dots + (a_{jP} \times Predictor P)\]</span></p>
<p>where <span class="math inline">\(a_{j1}\)</span>, <span class="math inline">\(a_{j2}\)</span>, …, <span class="math inline">\(a_{jP}\)</span> are component weights (or loading vectors).</p>
<p>Interpretations of the first principal component:</p>
<ul>
<li>The first principal component direction of the data is that along which the observations vary the most.</li>
<li>It defines the line that is as close as possible to the data. In other words, it minimizes the sum of the squared <em>perpendicular distances</em> between each point and the line.</li>
</ul>
<p>Together the first <span class="math inline">\(M\)</span> principal component score vectors and the first <span class="math inline">\(M\)</span> principal component loading vectors provide the best <span class="math inline">\(M\)</span>-dimensional approximation (in terms of Euclidean distance) to the <span class="math inline">\(i\)</span>th observation <span class="math inline">\(x_{ij}\)</span>.</p>
<p>To help PCA avoid summarizing distributional differences and predictor scale information, it is best to first transform skewed predictors and then center and scale them prior to performing PCA.</p>
<p>PCA is an <em>unsupervised</em> technique. If the predictive relationship between the predictors and response is not connected to the predictors’ variability, then the derived PCs will not provide a suitable relationship with the response. In this a case, a <em>supervised technique</em>, like PLS, will derive components while simultaneously considering the corresponding response (particularly, the first component is set to be the univariate regression coefficient by regressing the response variable against each explanatory variable).</p>
</div>
<div id="removing-near-zero-variance-predictors" class="section level4">
<h4>Removing near-zero variance predictors</h4>
<p>A rule of thumb for detecting near-zero variance predictors is (using <code>nearZeroVar</code>):</p>
<ul>
<li>The fraction of unique values over the sample size is low (e.g., 10%)</li>
<li>The ratio of the frequency of the most prevalent value to the frequency of the second most prevalent value is large (e.g., 20)</li>
</ul>
</div>
<div id="removing-collinearity" class="section level4">
<h4>Removing collinearity</h4>
<p>Using PCA to detect the magnitude of collinearity: if the first principal component accounts for a large % of the variance, this implies that there is at least one group of predictors that represent the same information.</p>
<p>In classical regression analysis, variance inflation factor (VIF) can be used to identify predictors that are impacted.</p>
<p><span class="math display">\[VIF(\hat{\beta_j}) = \frac{1}{1 - R^2_{X_j|X_{-j}}}\]</span></p>
<p>where <span class="math inline">\(R^2_{X_j|X_{-j}}\)</span> is the <span class="math inline">\(R^2\)</span> from a regression of <span class="math inline">\(X_j\)</span> onto all of the other <span class="math inline">\(X_j|X_{-j}\)</span> predictors. If <span class="math inline">\(R^2\)</span> is close to one, then collinearity is present, and so the VIF will be large.</p>
<p>A more heuristic approach is to remove the min number of predictors to ensure that all <em>pairwise</em> correlations are below a certain threshold (using <code>findCorrelation</code>):</p>
<ol style="list-style-type: decimal">
<li>Calculate the correlation matrix.</li>
<li>Determine the 2 predictors associated with the largest absolute pairwise correlation (i.e., predictors A and B).</li>
<li>Determine the average correlation between A and the other variables. Do the same for B.</li>
<li>If A has a larger average correlation, remove it; otherwise, remove B. 5 Repeat 1-4 until no absolute correlations are above the threshold.</li>
</ol>
</div>
</div>
<div id="resampling-techniques" class="section level3">
<h3>Resampling techniques</h3>
<ul>
<li>Stratified sampling using <code>createDataPartition</code></li>
<li>k-fold cross-validation</li>
<li>leave-one-out cross-validation: low bias, high variance</li>
<li>leave-group-out cross-validation</li>
<li>bootstrapping
<ul>
<li>On average, 63.2% of the data points in the bootstrap sample are represented at least once, so this technique has bias similar to k-fold cross-validation when k <span class="math inline">\(\approx\)</span> 2.</li>
<li>“632 method” to eliminate this bias: <span class="math display">\[(.632 \times \text{simple bootstrap estimate}) + (.368 \times \text{apparent error rate})\]</span> where the apparent error rate is the estimate from re-predicting the training set.</li>
</ul></li>
</ul>
</div>
<div id="regression-models" class="section level3">
<h3>Regression models</h3>
<div id="measuring-performance-in-regresson-models" class="section level4">
<h4>Measuring performance in regresson models</h4>
<ul>
<li><span class="math inline">\(R^2\)</span>: the proportion of variance explained. <span class="math display">\[R^2 = \frac{SSR}{SSY} = 1 - \frac{SSE}{SSY}\]</span> In a univariate regression, <span class="math inline">\(R^2\)</span> is the square of <span class="math inline">\(Cor(X, Y)\)</span>.</li>
<li><span class="math inline">\(\bar{R^2}\)</span> <span class="math display">\[\bar{R^2} = 1 - \frac{\frac{SSE}{n - k - 1}}{\frac{SSY}{n - 1}}\]</span></li>
<li>The variance-bias trade-off <span class="math display">\[MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y_i})^2\]</span> <span class="math display">\[E[MSE] = \sigma^2 + (\text{model bias})^2 + \text{model variance}\]</span> assuming the data points are statistically independent and that the residuals have a theoretical mean of 0 and a constant variance of <span class="math inline">\(\sigma^2\)</span>.
<ul>
<li><span class="math inline">\(\sigma^2\)</span> is the irreducible noise and cannot be eliminated by modeling.</li>
<li><span class="math inline">\((\text{model bias})^2\)</span> reflects how close the functional form of the model can get to the true relationship between the predictors and the outcome, or the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model, and is defined as: <span class="math display">\[bias(\hat{y_0})^2 = E[(y_0 - E(\hat{y_0}))^2]\]</span></li>
<li><span class="math inline">\(variance\)</span> refers to the amount by which <span class="math inline">\(\hat{y}\)</span> would change if we estimated it using a different training data set, and is defined as: <span class="math display">\[var(\hat{y_0}) = E[(E(\hat{y_0}) - \hat{y_0})^2]\]</span></li>
<li>As we increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance increases. Consequently, the expected test MSE declines. However, at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance.</li>
</ul></li>
</ul>
</div>
<div id="beta-estimates" class="section level4">
<h4><span class="math inline">\(\beta\)</span> estimates</h4>
<p><span class="math display">\[X \cdot \overrightarrow{\beta} = \overrightarrow{y}\]</span> <span class="math display">\[X^TX \cdot \overrightarrow{\beta} = X^T\overrightarrow{y}\]</span> <span class="math display">\[(X^TX)^{-1}X^TX \cdot \overrightarrow{\beta} = (X^TX)^{-1}X^T\overrightarrow{y}\]</span> <span class="math display">\[\overrightarrow{\beta} = (X^TX)^{-1}X^T\overrightarrow{y}\]</span></p>
<p>A unique inverse of the matrix <span class="math inline">\(X^TX\)</span> exists when</p>
<ol style="list-style-type: decimal">
<li>no predictor can be determined from a combination of one or more of the other predictors, and</li>
<li>the number of samples is greater than the number of predictors (possible remedies include removing pairwise correlated predictors, using VIF to diagnose multicollinearity, and using PCA to reduce the dimension of the predictor space).</li>
</ol>
<p>For univariate regressions, the coefficients can be calculated using sum of squares as follows: <span class="math display">\[\hat{\beta_1} = \frac{SSXY}{SSX} = \frac{\sum_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2}\]</span> <span class="math display">\[\hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x}\]</span></p>
<p>The standard errors of the <span class="math inline">\(\beta\)</span> estimates are: <span class="math display">\[SE(\beta_0)^2 = \sigma^2\Big[\frac{1}{n} + \frac{\bar{x}^2}{SSX}\Big] = \sigma^2\Big[\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n(x_i - \bar{x})^2}\Big]\]</span> <span class="math display">\[SE(\beta_1)^2 = \frac{\sigma^2}{SSX} = \frac{\sigma^2}{\sum_{i=1}^n(x_i - \bar{x})^2}\]</span> where <span class="math display">\[\sigma^2 = \frac{SSE}{n - 2}\]</span></p>
<p>Standard errors can be used to compute <em>confidence intervals</em>. A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter.</p>
</div>
<div id="f-test" class="section level4">
<h4>F test</h4>
<p><span class="math display">\[H_0: \beta_1 = \beta_2 = ... = \beta_p = 0\]</span> <span class="math display">\[H_1: \text{at least one }\beta\text{ is significantly different from 0}.\]</span> <span class="math display">\[F = \frac{\frac{SSR}{\text{model df}}}{\frac{SSE}{\text{error df}}} = \frac{\frac{SSY - SSE}{p}}{\frac{SSE}{n - p - 1}}\]</span> The F-statistic relies on the normality assumption, but even if the errors are not normally-distributed, the F-statistic approximately follows an F-distribution provided that the sample size <span class="math inline">\(n\)</span> is large.</p>
<p>To test whether a particular variable, or a particular group of variables, are significant, use F test to test whether excluding them causes a significant increase in deviance: <span class="math display">\[F = \frac{\frac{SSE_{restricted} - SSE_{unrestricted}}{\Delta_{df}}}{\frac{SSE}{n - p - 1}}\]</span> When only one variable is tested, the square of its t-statistic is the corresponding F-statistic.</p>
</div>
<div id="problems-with-regression-models" class="section level4">
<h4>Problems with regression models</h4>
<ul>
<li>Non-linearity of the data
<ul>
<li>Residual plots are a useful graphical tool for identifying non-linearity.</li>
</ul></li>
<li>Correlation of error terms (autocorrelation)
<ul>
<li>If there is correlation among the error terms, then the estimated standard errors will tend to underestimate the true standard errors.</li>
<li>Methods to detect autocorrelation:
<ul>
<li>Durbin-watson test <span class="math display">\[d = {\sum_{t=2}^T (e_t - e_{t-1})^2 \over {\sum_{t=1}^T e_t^2}} \approx 2(1 - \rho)\]</span> where <span class="math inline">\(\rho\)</span> is the sample autocorrelation of the residuals.</li>
<li>Plot residuals, use <code>acf</code></li>
</ul></li>
</ul></li>
</ul>
<pre class="r"><code>suppressMessages(library(dplyr))
suppressMessages(library(quantmod))
suppressMessages(library(lmtest))
options(&#39;getSymbols.warning4.0&#39; = F)

getSymbols(&#39;AAPL&#39;)</code></pre>
<pre><code>## [1] &quot;AAPL&quot;</code></pre>
<pre class="r"><code>aapl = data.frame(AAPL)
aapl$date = row.names(aapl)
aapl_return = diff(aapl$AAPL.Adjusted) / aapl$AAPL.Adjusted[-nrow(aapl)]
aapl$aapl_ret = c(NA, aapl_return)
aapl = select(aapl, date, aapl_ret)

getSymbols(&#39;SPX&#39;)</code></pre>
<pre><code>## [1] &quot;SPX&quot;</code></pre>
<pre class="r"><code>spx = data.frame(SPX)
spx$date = row.names(spx)
spx_return = diff(spx$SPX.Adjusted) / spx$SPX.Adjusted[-nrow(spx)]
spx$spx_ret = c(NA, spx_return)
spx = select(spx, date, spx_ret)

returns = inner_join(x = aapl, y = spx, by = &#39;date&#39;)
returns = filter(returns, date &gt;= as.Date(&#39;2012-01-01&#39;))
model = lm(aapl_ret ~ spx_ret, data = returns)

resid = residuals(model)
plot.ts(resid)</code></pre>
<p><img src="/post/2014-11-15-notes-to-machine-learning_files/figure-html/acf-1.png" width="672" /></p>
<pre class="r"><code>acf(resid)</code></pre>
<p><img src="/post/2014-11-15-notes-to-machine-learning_files/figure-html/acf-2.png" width="672" /></p>
<pre class="r"><code>dwtest(model)</code></pre>
<pre><code>## 
##  Durbin-Watson test
## 
## data:  model
## DW = 1.9673, p-value = 0.3579
## alternative hypothesis: true autocorrelation is greater than 0</code></pre>
<pre class="r"><code>par(mfrow = c(2, 2))
plot(model)</code></pre>
<p><img src="/post/2014-11-15-notes-to-machine-learning_files/figure-html/acf-3.png" width="672" /></p>
<ul>
<li>Non-constant variance of error terms (heteroskedasticity)
<ul>
<li>Methods to detect heterokedasticity:
<ul>
<li>Breusch-Pagan test, White test</li>
<li>Residual plot</li>
</ul></li>
<li>Methods to correct heteroskedasticity:
<ul>
<li>Transform the response <span class="math inline">\(Y\)</span> using a concave function such as <span class="math inline">\(\text{log}Y\)</span> or <span class="math inline">\(\sqrt{Y}\)</span>. Such a transformation results in a greater amount of shrinkage of the larger responses, leading to a reduction in heteroscedasticity.</li>
<li>If the form of the heteroskedasticity is known, we can use WLS to assign less weight to observations with a higher error variance.
<ul>
<li>For example, the <span class="math inline">\(i\)</span>th response could be an average of <span class="math inline">\(n_i\)</span> raw observations. If each of these raw observations is uncorrelated with variance <span class="math inline">\(\sigma^2\)</span>, then their average has variance <span class="math inline">\(\sigma_i^2 = \frac{\sigma^2}{n_i}\)</span>. In this case a simple remedy is to fit our model using weights proportional to the inverse variances, i.e., <span class="math inline">\(w_i = n_i\)</span> in this case.</li>
</ul></li>
</ul></li>
</ul></li>
<li>Outliers (in the y-direction)
<ul>
<li>A point for which <span class="math inline">\(y_i\)</span> is far from the value predicted by the mode.</li>
<li>Residual plots can be used to identify outliers, but instead of using residuals themselves, we can plot the <em>studentized residuals</em>, computed by dividing each residual by its estimated standard error. Observations whose studentized residuals are greater than 3 in absolute value are possible outliers.</li>
<li>Linear regression is prone to chasing observations that are away from the overall trend of the majority of the data. To cure this problem, alternative metrics to SSE can be used:
<ul>
<li>minimizing the sum of the absolute errors</li>
<li>Huber function (using the squared residuals when they are small and the absolute ones when they are above a threshold)</li>
</ul></li>
</ul></li>
<li>High leverage points (in the x-direction)
<ul>
<li>Methods to detect high leverage points:
<ul>
<li>Leverage statistic
<ul>
<li>The <span class="math inline">\(i\)</span>th diagonal element of the hat matrix <span class="math inline">\(\mathbf{X}\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\)</span>: <span class="math display">\[h_i = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum(x_i - \bar{x})^2}\]</span></li>
<li>The leverage statistic <span class="math inline">\(h_i\)</span> is always between <span class="math inline">\(\frac{1}{n}\)</span> and 1, and the average leverage for all the observations is always equal to <span class="math inline">\(\frac{p + 1}{n}\)</span>. Hence, if a given observation has a leverage statistic that greatly exceeds <span class="math inline">\(\frac{p + 1}{n}\)</span>, then we may suspect that the corresponding point has high leverage.</li>
</ul></li>
<li>Cook’s distance
<ul>
<li>Combines both leverage and outliers: <span class="math display">\[C_i = \frac{1}{p}r_{studentized}^2\frac{h_i}{1 - h_i}\]</span></li>
</ul></li>
</ul></li>
</ul></li>
<li>Collinearity
<ul>
<li>Collinearity reduces the accuracy of the estimates of the regression coefficients, and, hence, causes the standard error for <span class="math inline">\(\beta_j\)</span> to grow.</li>
<li>Method to detect collinearity: VIF</li>
<li>Method to correct collinearity: orthoganization, PCA, heuristic approach</li>
</ul></li>
</ul>
</div>
<div id="paritial-least-squares" class="section level4">
<h4>Paritial least squares</h4>
<ul>
<li>PCR: pre-processing predictors via PCA prior to performing regression (<em>unsupervised</em>)</li>
<li>PLS: while the PCA linear combinations are chosen to maximally summarize predictor space variability, the PLS linear combinations of predictors are chosen to maximally summarize covariance with the response (<em>supervised</em>). This means that PLS finds components that maximally summarize the variation of the predictors while simultaneously requiring these components to have max correlation with the response.</li>
</ul>
</div>
<div id="penalized-models-to-prevent-overfitting" class="section level4">
<h4>Penalized models to prevent overfitting</h4>
<ul>
<li>Ridge regression: <span class="math display">\[SSE_{L2} = \sum_{i=1}^{n}(y_i - \hat{y_i})^2 + \lambda\sum_{j=1}^{P}\beta^2\]</span></li>
<li>Lasso regression: <span class="math display">\[SSE_{L1} = \sum_{i=1}^{n}(y_i - \hat{y_i})^2 + \lambda\sum_{j=1}^{P}|\beta|\]</span>
<ul>
<li>Ridge regression is known to shrink the coefficients of correlated predictors towards each other, allowing them to borrow strength from each other. in the extreme case of k identical predictors, they each get identical coefficients with <span class="math inline">\(\frac{1}{k}\)</span>th the size that any single one would get if fit alone.</li>
<li>Lasso, on the other hand, is somewhat indifferent to very correlated predictors, and will tend to pick one and ignore the rest (i.e., feature selection).</li>
</ul></li>
<li>Elastic net: <span class="math display">\[SSE_{Enet} = \sum_{i=1}^{n}(y_i - \hat{y_i})^2 + \lambda_1\sum_{j=1}^{P}\beta^2 + \lambda_2\sum_{j=1}^{P}|\beta|\]</span></li>
<li>Predictor data need to be centered and scaled first before applying regularization.</li>
</ul>
</div>
</div>
<div id="smoothing" class="section level3">
<h3>Smoothing</h3>
<div id="regression-spline" class="section level4">
<h4>Regression spline</h4>
<ul>
<li>Idea: instead of fitting a high-degree polynomial over the entire range of <span class="math inline">\(X\)</span>, <em>piece-wise polynomial regression</em> involves fitting separate low-degree polynomials over different regions of <span class="math inline">\(X\)</span>.</li>
<li>Cubic spline:
<ul>
<li>If we place <span class="math inline">\(K\)</span> different knots throughout the range of <span class="math inline">\(X\)</span>, then we will end up fitting <span class="math inline">\(K + 1\)</span> different cubic polynomials.</li>
<li>The regression spline is most flexible in regions that contain a lot of knots.</li>
<li>3 additional constraints are added for cubic spline: at each knot, the fitted curve must be continuous, and so must its first and second derivatives (i.e., smooth). Hence, a cubic spline with <span class="math inline">\(K\)</span> knots uses a total of <span class="math inline">\(K + 4\)</span> degrees of freedom.</li>
<li>Each constraint that we impose on the piecewise cubic polynomials effectively frees up one degree of freedom, by reducing the complexity of the resulting piecewise polynomial fit. <span class="math display">\[y_i = \beta_0 + \beta_1b_1(x_i) + \beta_2b_2(x_i) +...+ \beta_{K+3}b_{K+3}(x_i) + \epsilon_i\]</span> where <span class="math inline">\(b_i\)</span> is the basis function. Typically, <span class="math display">\[y_i = \beta_0 + x + x^2 + x^3 + h(x, \xi_1) + h(x, \xi_2) +...+ h(x, \xi_K)\]</span> where <span class="math display">\[h(x, \xi) = (x - \xi)_+^3\]</span></li>
</ul></li>
<li>Natural spline:
<ul>
<li>Additional boundary constraints: the function is required to be linear at the boundary (in the region where X is smaller than the smallest knot, or larger than the largest knot).</li>
</ul></li>
</ul>
</div>
<div id="smoothing-spline" class="section level4">
<h4>Smoothing spline</h4>
<ul>
<li>Minimize <span class="math display">\[\sum_{i=1}^n(y_i - g(x_i))^2 + \lambda\int g&#39;&#39;(t)^2dt\]</span> where <span class="math inline">\(\lambda\)</span> is a nonnegative tuning parameter and <span class="math inline">\(\lambda\int g&#39;&#39;(t)^2dt\)</span> penalizes the variability in <span class="math inline">\(g\)</span>.</li>
<li>The function <span class="math inline">\(g(x)\)</span> that minimizes it is a natural cubic spline with knots at <span class="math inline">\(x_1, ..., x_n\)</span>.</li>
<li>As <span class="math inline">\(\lambda\)</span> increases from 0 to <span class="math inline">\(\infty\)</span>, the effective degrees of freedom decrease from <span class="math inline">\(n\)</span> to 2.</li>
</ul>
</div>
<div id="local-regression" class="section level4">
<h4>Local regression</h4>
<ul>
<li>Computes the fit at a target point <span class="math inline">\(x_0\)</span> using only the nearby training observations.</li>
<li>Needs to determine the <em>span</em>, <span class="math inline">\(s = \frac{k}{n}\)</span>, which is the fraction of training points whose <span class="math inline">\(x_i\)</span> are closest to <span class="math inline">\(x_0\)</span>. The smaller the value of <span class="math inline">\(s\)</span>, the more local and wiggly will be our fit; alternatively, a very large value of <span class="math inline">\(s\)</span> will lead to a global fit to the data using all of the training observations.</li>
</ul>
</div>
<div id="generalized-additive-models" class="section level4">
<h4>Generalized additive models</h4>
<ul>
<li>A general framework for extending a standard linear model by allowing non-linear functions of each of the variables, while maintaining additivity. <span class="math display">\[y_i = \beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + \dots + f_p(x_{ip}) + \epsilon_i\]</span> where <span class="math inline">\(f_j(x_{ij})\)</span> is the non-linear functions.</li>
<li>GAMs provide a useful compromise between linear and fully nonparametric models.</li>
</ul>
</div>
<div id="gams-for-logistic-regression" class="section level4">
<h4>GAMs for logistic regression</h4>
<p><span class="math display">\[log\Big(\frac{p}{1 - p}\Big) = \beta_0 + f_1(x_1) + \dots + f_p(x_P)\]</span></p>
</div>
<div id="multivariate-adaptive-regression-splines-mars" class="section level4">
<h4>Multivariate adaptive regression splines (MARS)</h4>
<ul>
<li>MARS creates a <em>piecewise linear model</em> where each new surrogate feature models an isolated portion of the original data.</li>
<li>Each data points for each predictor is evaluated as a candidate cut point by creating a linear regression model with the candidate features, and the corresponding model error is calculated. The predictor/cut point combination that achieves the smallest error is used (as <span class="math inline">\(h(x - a)\)</span> and <span class="math inline">\(h(a - x)\)</span>).</li>
<li>After the initial model is created with the first 2 features, the model conducts another exhaustive search to find the next set of features that, given the initial set, yield the best model fit.</li>
<li>Pruning: once the full set of features has been created, the algorithm sequentially removes individual features that do not contribute significantly to the model equation (in terms of decreases in error rate) using <em>generalized cross-validation</em>.</li>
<li>Another pruning parameter is for the degree of the features that are added to the model (i.e., interaction terms).</li>
<li>Extension to classification: FDA</li>
</ul>
</div>
</div>
<div id="neural-networks" class="section level3">
<h3>Neural networks</h3>
<ul>
<li>Hidden units: <span class="math display">\[h_k(X) = g\Big(\beta_{0k} + \sum_{i=1}^{P}x_j\beta_{jk}\Big)\]</span> where <span class="math display">\[g(u) = \frac{1}{1 + e^{-u}}\]</span> and <span class="math inline">\(k\)</span> is the number of hidden units in a layer</li>
<li>Combining hidden units (single-layer): <span class="math display">\[f(x) = \gamma_0 + \sum_{k=1}^{H}\gamma_kh_k\]</span></li>
<li>The back-propagation algorithm uses derivatives to find the optimal parameters. However, it is common that a solution is not a global solution.</li>
<li>Regularization using <em>weight decay</em> (single-layer): <span class="math display">\[\sum_{i=1}^{n}(y_i - f_i(x))^2 + \lambda\sum_{k=1}^{H}\sum_{j=0}^{P}\beta_{jk}^2 + \lambda\sum_{k=0}^{H}\gamma_k^2\]</span></li>
</ul>
<div id="for-classification" class="section level4">
<h4>For classification</h4>
<ul>
<li>The softmax transformation is used to ensure the outputs of the neural network lie between 0 and 1: <span class="math display">\[f_{il}^*(x) = \frac{e^{f_{il}(x)}}{\sum_le^{f_{il}(x)}}\]</span> where <span class="math inline">\(f_{il}(x)\)</span> is the model prediction of the <span class="math inline">\(l\)</span>th class and the <span class="math inline">\(i\)</span>th sample.</li>
<li>Optimization criteria:
<ul>
<li>minimize sum of squares of errors: <span class="math display">\[\sum_{l=1}^C\sum_{i=1}^n(y_{il} - f_{il}^*(x))^2\]</span></li>
<li>maximize entropy: <span class="math display">\[\sum_{l=1}^C\sum_{i=1}^ny_{il}\text{ln}f_{il}^*(x)\]</span> although studies have shown that differences in performance tend to be negligible, the entropy function should more accurately estimate small probabilities than those generated by the squared-error function.</li>
</ul></li>
</ul>
</div>
</div>
<div id="support-vector-machines" class="section level3">
<h3>Support vector machines</h3>
<div id="hyperplane" class="section level4">
<h4>Hyperplane</h4>
<ul>
<li>In a <span class="math inline">\(p\)</span>-dimensional space, a hyperplane is a flat affine subspace of dimension <span class="math inline">\(p - 1\)</span>. The word <em>affine</em> indicates that the subspace need not pass through the origin. <span class="math display">\[\beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_pX_p = 0\]</span></li>
<li>The maximal margin hyperplane is the separating hyperplane for which the margin is largest—that is, it is the hyperplane that has the farthest minimum distance to the training observations.</li>
<li>The maximal margin hyperplane depends directly on the <em>support vectors</em>, but not on the other observations.</li>
</ul>
</div>
<div id="svm-for-classificatoin" class="section level4">
<h4>SVM for classificatoin</h4>
<ul>
<li>Optimization: <span class="math display">\[max_{\beta_0, \beta_1, \dots, \beta_p}M\]</span> <span class="math display">\[\text{subject to }\sum_{j=1}^p\beta_j^2 = 1\]</span> <span class="math display">\[y_i(\beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \dots + \beta_px_{ip}) \ge M \text{  } \forall i = 1, 2, \dots, n.\]</span>
<ul>
<li>The first constraint ensure that the perpendicular distance from the <span class="math inline">\(i\)</span>th observation to the hyperplane is given by <span class="math display">\[y_i(\beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \dots + \beta_px_{ip})\]</span></li>
<li>The 2 constraints ensure that each observation is on the correct side of the hyperplane and at least a distance M from the hyperplane.</li>
</ul></li>
<li>Sometimes a a separating hyperplane doesn’t exist; even if it does, it may lead to sensitivity to individual observations. Hence, an alternative classifier that allows misclassification solves the following optimization problem: <span class="math display">\[max_{\beta_0, \beta_1, \dots, \beta_p, \epsilon_1, \epsilon_2, \dots, \epsilon_n}M\]</span> <span class="math display">\[\text{subject to }\sum_{j=1}^p\beta_j^2 = 1\]</span> <span class="math display">\[y_i(\beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \dots + \beta_px_{ip}) \ge M(1 - \epsilon_i)\]</span> <span class="math display">\[\epsilon_i \ge 0, \sum_{i=1}^n\epsilon_i \le C\]</span>
<ul>
<li><span class="math inline">\(\epsilon_1, \epsilon_2, \dots, \epsilon_n\)</span> are <em>slack variables</em> that allow individual observations to be on the wrong side of the margin or the hyperplane.
<ul>
<li>If <span class="math inline">\(\epsilon_i &gt; 0\)</span> then the <span class="math inline">\(i\)</span>th observation is on the wrong side of the margin.</li>
<li>If <span class="math inline">\(\epsilon_i &gt; 1\)</span> then it is on the wrong side of the hyperplane.</li>
</ul></li>
<li><span class="math inline">\(C\)</span> is the budget for the amount that the margin can be violated by the <span class="math inline">\(n\)</span> observations. As the budget <span class="math inline">\(C\)</span> increases, we become more tolerant of violations to the margin, and so the margin will widen.</li>
<li>Observations that lie directly on the margin, or on the wrong side of the margin for their class, are support vectors and affect the support vector classifier. Hence, SVM is quite robust to the behavior of observations that are far away from the hyperplane.</li>
</ul></li>
<li>The classification function for a new sample can be written as <span class="math display">\[f(\mathbf{u}) = \beta_0 + \mathbf{\beta&#39;u}
            = \beta_0 + \sum_{j=1}^P\beta_ju_j
            = \beta_0 + \sum_{i=1}^ny_i\alpha_ix_i&#39;\mathbf{u}\]</span> where <span class="math inline">\(\alpha_i\)</span> is nonzero only for the support vectors.</li>
<li>The dot product, <span class="math inline">\(x_i&#39;\mathbf{u}\)</span>, can be written as a product of the distance of <span class="math inline">\(x_i\)</span> from the origin, the distance of <span class="math inline">\(\mathbf{u}\)</span> from the origin, and the cosine of the angle between <span class="math inline">\(x_i\)</span> and <span class="math inline">\(\mathbf{u}\)</span>. In other words, it measures the similarity between <span class="math inline">\(x_i\)</span> and <span class="math inline">\(\mathbf{u}\)</span>.</li>
<li>More generally, given the <em>dot product</em> of the new samples and the training samples, the equation can be written as: <span class="math display">\[f(\mathbf{u}) = \beta_0 + \sum_{i=1}^ny_i\alpha_iK(\mathbf{x_i}, \mathbf{u})\]</span> where <span class="math inline">\(K(\cdot)\)</span> is called the <em>kernal function</em>, which quantifies the similarity of two observations. Other types of kernel functions can be used to encompass nonlinear functions of the predictors: <span class="math display">\[\text{polynomial} = (\phi(\mathbf{x&#39;u}) + 1)^{degree}\]</span> <span class="math display">\[\text{radial basis function} = \exp(-\sigma||\mathbf{x} - \mathbf{u}||^2)\]</span> <span class="math display">\[\text{hyperbolic tangent} = \tanh(\phi(\mathbf{x&#39;u}) + 1)\]</span> where <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\sigma\)</span> are scaling parameters.</li>
</ul>
</div>
<div id="relationship-with-logistic-regression" class="section level4">
<h4>Relationship with logistic regression</h4>
<ul>
<li>It can be shown that optimization criterion for SVM can be rewritten to the form of “Loss + Penalty” as follows: <span class="math display">\[minimize_{\beta_0, \beta_1, \dots, \beta_p}{\sum_{i=1}^nmax[0, 1 - y_if(x_i)] + \lambda\sum_{j=1}^p\beta_j^2}\]</span></li>
<li>The <em>hinge loss</em> function is closely related to the loss function used in logistic regression as shown below:</li>
</ul>
<pre class="r"><code>library(reshape2)
library(ggplot2)
yx = seq(-5, 4, .01)
l1 = log(1 + exp(-yx))
l2 = ifelse(1 - yx &lt; 0, 0, 1 - yx)
df = data.frame(cbind(yx, l1, l2))
df = melt(df, id = &#39;yx&#39;)
ggplot(df, aes(x = yx, y = value, colour = variable)) + geom_line() + 
scale_colour_discrete(name = &#39;Model&#39;, labels = c(&#39;Logistic Regression Loss&#39;, &#39;SVM Loss&#39;)) + xlab(&#39;yf(x)&#39;) + ylab(&#39;Loss&#39;)</code></pre>
<p><img src="/post/2014-11-15-notes-to-machine-learning_files/figure-html/loss-functions-1.png" width="672" /></p>
<ul>
<li>Due to the similarities between their loss functions, logistic regression and the support vector classifier often give very similar results. When the classes are well separated, SVMs tend to behave better than logistic regression; in more overlapping regimes, logistic regression is often preferred.</li>
</ul>
</div>
<div id="svm-for-regression" class="section level4">
<h4>SVM for regression</h4>
<ul>
<li><span class="math inline">\(\epsilon\)</span>-insensitive loss function:
<ul>
<li>data points with residuals within the threshold (i.e., samples that the model fits well) do not contribute to the regression fit.</li>
<li>data points with an absolute difference greater than the threshold contribute a linear-scale amount.</li>
</ul></li>
<li>As a result, the poorly-predicted points (the “outliers” in the extreme case) are the only points that define the regression line.</li>
<li>The SVM regression coefficients minimize <span class="math display">\[cost\sum_{i=1}^{n}L_\epsilon(y_i - \hat{y_i}) + \sum_{j=1}^{P}\beta_j^2\]</span> where <span class="math inline">\(L_\epsilon(\cdot)\)</span> is the <span class="math inline">\(\epsilon\)</span>-insensitive function and <span class="math inline">\(cost\)</span> is the cost penalty as the reverse of the <span class="math inline">\(\lambda\)</span> used in regularized regressions or neural networks.</li>
<li>The linear SVM predictor function for a new sample <span class="math inline">\(u\)</span> is written as: <span class="math display">\[\hat{y} = \beta_0 + \beta_1u_1 + \dots + \beta_Pu_P
      = \beta_0 + \sum_{j=1}^{P}\beta_ju_j
      = \beta_0 + \sum_{j=1}^{P}\sum_{i=1}^{n}\alpha_ix_{ij}u_j
      = \beta_0 + \sum_{i=1}^{n}\alpha_i\Big(\sum_{j=1}^{P}x_{ij}u_j\Big)\]</span></li>
<li>Each data points are used in the prediction, however,</li>
<li>For training set samples that are within <span class="math inline">\(\pm\epsilon\)</span> of the regression line, the <span class="math inline">\(\alpha\)</span> parameters are 0, indicating they have no impact on prediction.</li>
<li>The rest of the data points with non-zero <span class="math inline">\(\alpha\)</span> are called the <em>support vectors</em>.</li>
</ul>
</div>
</div>
<div id="k-nearest-neighbors" class="section level3">
<h3>K-nearest neighbors</h3>
<ul>
<li>For classification, KNN estimates the conditional probability for class <span class="math inline">\(j\)</span> as the fraction of points in <span class="math inline">\(N_0\)</span> whose response values equal <span class="math inline">\(j\)</span>: <span class="math display">\[Pr(Y = j|X = x_0) = \frac{1}{K}\sum_{i \in N_0}I(y_i = j)\]</span> It then applies Bayes rule and classifies the test observation <span class="math inline">\(x_0\)</span> to the class with the largest probability.</li>
<li>For regression, KNN estimates <span class="math inline">\(f(x_0)\)</span> using the average of all the training responses in <span class="math inline">\(N_0\)</span>: <span class="math display">\[\hat{f(x_0)} = \frac{1}{K}\sum_{i \in N_0}y_i\]</span></li>
<li>Predictor data need to be centered and scaled prior to performing KNN.</li>
<li>Also need to remove irrelevant or noisy predictors.</li>
<li>Suffers from curse of dimensionality: the <span class="math inline">\(K\)</span> observations that are nearest to a given test observation <span class="math inline">\(x_0\)</span> may be very far away from <span class="math inline">\(x_0\)</span> in <span class="math inline">\(p\)</span>-dimensional space when <span class="math inline">\(p\)</span> is large.</li>
</ul>
</div>
<div id="trees" class="section level3">
<h3>Trees</h3>
<div id="regression-trees" class="section level4">
<h4>Regression trees</h4>
<ul>
<li>SSE is used to find the optimal split.</li>
<li>Each terminal node uses the average of the training set outcomes in that node for prediction.</li>
<li>The top-down approach (aka, <em>recursive binary splitting</em>) is greedy because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.</li>
<li>We consider all predictors <span class="math inline">\(X_1, \dots, X_p\)</span>, and all possible values of the cutpoint for each of the predictors, and then choose the predictor and cutpoint such that the resulting tree has the lowest SSE.</li>
<li>Pruning to prevent overfitting: apply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of <span class="math inline">\(C_p\)</span>. <span class="math display">\[SSE_{C_p} = SSE + C_p \times (\text{number of terminal nodes})\]</span></li>
<li>Handling missing data:
<ul>
<li>When building the tree, missing data are ignored.</li>
<li>For each split, a variety of alternatives, <em>surrogate splits</em>, are evaluated. A surrogate split is one whose results are similar to the original split actually used in the tree.</li>
<li>If a surrogate split approximates the original split well, it can be used when the predictor data associated with the original split are not available.</li>
</ul></li>
<li>Variable importance: overall reduction in the optimization criteria for each predictor.</li>
<li>Limitations:
<ul>
<li>By construction, tree models partition the data into rectangular regions of the predictor space, which may result in sub-optimal predictive performance.</li>
<li>An individual tree tends to be unstable. If the data are slightly altered, a completely different set of splits might be found. Ensemble methods, on the other hand, exploit this characteristic to create models that tend to have extremely good performance.</li>
<li>Trees suffer from <em>selection bias</em>: predictors with a higher number of distinct values (including continuous variables) are favored over more granular predictors. Also, as the number of missing values increases, the selection of predictors becomes more biased.</li>
<li>Correlations between predictors can dilute the importance of key predictors.</li>
</ul></li>
<li>Conditional inference trees: for a candidate split, statistical hypothesis test is used to evaluate the difference between the means of the 2 groups created by the split and a p-value is computed.</li>
</ul>
</div>
<div id="regression-model-trees" class="section level4">
<h4>Regression model trees</h4>
<ul>
<li>Splitting criterion: <span class="math display">\[\text{reduction} = SD(S) - \sum_{i=1}^{P}\frac{n_i}{n} \times SD(S_i)\]</span> where SD is the standard deviation of the dataset (<span class="math inline">\(S\)</span> denotes the entire set and <span class="math inline">\(S_i\)</span> denotes the <span class="math inline">\(i\)</span>th subset) and <span class="math inline">\(n_i\)</span> is the number of samples in partition <span class="math inline">\(i\)</span>. Hence, it determines if the total variation in the splits, weighted by sample size, is lower than in the presplit data. In subsequent split, the error associated with each linear model is used in place of <span class="math inline">\(SD(S)\)</span>.</li>
<li>For a given linear model, an adjusted error rate is computed as follows: <span class="math display">\[\text{adjusted error rate} = \frac{n^* + p}{n^* - p}\sum_{i=1}^{n^*}|y_i - \hat{y_i}|\]</span> where <span class="math inline">\(n^*\)</span> is the number of training set data points used to build the model and <span class="math inline">\(p\)</span> is the number of parameters.</li>
<li>Once the complete set of models have been created, each undergoes a simplification procedure to potentially drop some of the terms. Terms are dropped as long as the adjusted error rate decreases.</li>
<li>Smoothing: when predicting, the new sample goes down the appropriate path, and moving from the bottom up, the linear models along that path are combined. Each time 2 predictors are combined using <span class="math display">\[\hat{y} = \frac{n_k\hat{y_k} + c\hat{y_p}}{n_k + c}\]</span> where <span class="math inline">\(y_k\)</span> and <span class="math inline">\(y_p\)</span> are the predictions from the child and parent nodes, respectively, <span class="math inline">\(n_k\)</span> is the number of training set points in the child node, and <span class="math inline">\(c\)</span> is a constant with a default value of 15.</li>
<li>Pruning is done using the adjusted error rate.</li>
</ul>
</div>
<div id="classification-trees" class="section level4">
<h4>Classification trees</h4>
<ul>
<li>For a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs.</li>
<li>Splitting criteria:
<ul>
<li>Classification error rate: the fraction of the training observations in that region that do not belong to the most common class. <span class="math display">\[E = 1 - max_k(\hat{p_{mk}})\]</span> where <span class="math inline">\(\hat{p_{mk}}\)</span> represents the proportion of training observations in the <span class="math inline">\(m\)</span>th region that are from the <span class="math inline">\(k\)</span>th class. However, it turns out that classification error is not sufficiently sensitive for tree-growing, and in practice two other measures are preferable.</li>
<li>Gini index: <span class="math display">\[G = \sum_{k=1}^K\hat{p_{mk}}(1 - \hat{p_{mk}})\]</span> Gini index is a measure of node <em>purity</em> as a small value indicates that a node contains predominantly observations from a single class.</li>
<li>Cross-entropy: <span class="math display">\[D = -\sum_{k=1}^K\hat{p_{mk}}\text{log}\hat{p_{mk}}\]</span> Like the Gini index, the cross-entropy will take on a small value if the <span class="math inline">\(m\)</span>th node is pure. In fact, it turns out that the Gini index and the cross-entropy are quite similar numerically.</li>
</ul></li>
</ul>
</div>
<div id="rule-based-models" class="section level4">
<h4>Rule-based models</h4>
<ul>
<li>A rule is defined as a distinct path through a tree. One approach to creating rules from model trees is
<ol style="list-style-type: decimal">
<li>An initial model tree is created.</li>
<li>Only the rule with the largest coverage is saved form this model. The samples covered by the rule are removed from the training set and another model tree is created with the remaining data.</li>
<li>Repeat until all the training data have been covered by at least 1 rule.</li>
</ol></li>
</ul>
</div>
</div>
<div id="random-forests" class="section level3">
<h3>Random forests</h3>
<ul>
<li>Tree correlation prevents bagging from optimally reducing variance of the predicted values.</li>
<li>Random forests randomly selects predictors (e.g., <span class="math inline">\(\sqrt{p}\)</span>) at each split, hence reducing tree correlation.</li>
<li>To produce stable results, in practice at least 1,000 trees are needed.</li>
<li>Because each learner is selected independently of all previous learners, random forests is robust to a noisy response.</li>
<li><em>Out-of-bag</em> error rate can be used to assess the predictive performance.</li>
<li>Variable importance is determined by aggregating the individual improvement values for each predictor across the forest.</li>
</ul>
</div>
<div id="gradient-boosting-trees" class="section level3">
<h3>Gradient boosting trees</h3>
<ul>
<li>Boosting differs from bagging in that the trees are grown sequentially: each tree is grown using information from previously grown trees. Hence, boosting does <em>not</em> involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.</li>
<li>Tuning parameters: tree depth (or interaction depth), number of iterations, and learning rate <span class="math inline">\(\lambda\)</span>.
<ul>
<li>Tree depth, or the number of splits, controls the complexity of the boosted ensemble (often <span class="math inline">\(d = 1\)</span> works well, in which case each tree is a stump, consisting of a single split). This highlights one difference between boosting and random forests: in boosting, because the growth of a particular tree takes into account the other trees that have already been grown, smaller trees are typically sufficient. Using smaller trees can aid in interpretability as well; for instance, using stumps leads to an additive model.</li>
<li>Unlike bagging and random forests, boosting can overfit if the number of trees/iterations is too large.</li>
<li>Updating the predicted value of each sample by adding the value predicted in the current iteration to that of the previous iteration. To prevent overfitting, only a fraction of the current predicted value, as determined by <span class="math inline">\(\lambda\)</span> is added. In general, statistical learning approaches that learn slowly tend to perform well.</li>
</ul></li>
<li>Using the residuals as the response to fit a tree. By fitting small trees to the residuals, we slowly improve <span class="math inline">\(\hat{f}\)</span> in areas where it does not perform well. <span class="math display">\[\hat{f(x)} \leftarrow \hat{f(x)} + \lambda\hat{f^b(x)}\]</span> <span class="math display">\[r_i \leftarrow r_i - \lambda\hat{f^b(x)}\]</span> <span class="math display">\[\hat{f(x)} = \sum_{b=1}^B\lambda\hat{f^b(x)}\]</span></li>
<li>Stochastic gradient boosting: randomly select a fraction of the training data before constructing gradient boost trees.</li>
</ul>
</div>
<div id="cubist" class="section level3">
<h3>Cubist</h3>
<ul>
<li>Cubist is a rule-based model.</li>
<li>Smoothing is done as follows: <span class="math display">\[\hat{y} = a \times \hat{y_k} + (1 - a) \times \hat{y_p}\]</span> where <span class="math display">\[a = \frac{var(e_p) - cov(e_k, e_p)}{var(e_p - e_k)}\]</span> where <span class="math inline">\(e_k\)</span> and <span class="math inline">\(e_p\)</span> are the residuals of the child and parent models, respectively. In the end, the model with the smallest RMSE has a higher weight in the smoothed model.</li>
<li>Boosting(committee model): similar to GBM, the <span class="math inline">\(m\)</span>th committee model uses an adjusted response calculated as: <span class="math display">\[y_m^* = y - (\hat{y_{m-1}} - y)\]</span> Hence, if a data point is underpredicted, the sample value is increased in the hope that the model will produce a larger prediction in the next iteration, and vice versa.</li>
<li>Finally, when predicting a new model, the <span class="math inline">\(K\)</span> most similar neighbors are determined from the training set and their observed outcome and predicted values are used to adjust the prediction as: <span class="math display">\[\frac{1}{K}\sum_{l=1}^{K}w_l[t_l + (\hat{y} - \hat{t_l})]\]</span> where <span class="math inline">\(t_l\)</span> and <span class="math inline">\(\hat{t_l}\)</span> are the observed and prediction for a training set neighbor, and <span class="math inline">\(w_l\)</span> is a weight calculated using the distance of the neighbor to the new sample.</li>
<li>Tuning parameters: number of committees and neighbors.</li>
</ul>
</div>
<div id="measuring-performace-in-classification-models" class="section level3">
<h3>Measuring performace in classification models</h3>
<ul>
<li>Overall accuracy</li>
<li>Kappa statistic: <span class="math display">\[Kappa = \frac{O - E}{1 - E}\]</span> where <span class="math inline">\(O\)</span> is the observed accuracy and <span class="math inline">\(E\)</span> is the expected accuracy based on the marginal totals of the confusion matrix.</li>
<li>Sensitivity: true positive rate</li>
<li>Specificity: true negative rate = 1 - false positive rate</li>
</ul>
</div>
<div id="linear-classificatoin-models" class="section level3">
<h3>Linear classificatoin models</h3>
<div id="logistic-regression" class="section level4">
<h4>Logistic regression</h4>
<ul>
<li>Binomial likelihood: <span class="math display">\[L(p) = C_n^r p^r (1 - p)^{n-r}\]</span> where <span class="math inline">\(n\)</span> is total number of trials and <span class="math inline">\(r\)</span> is the number of successes.</li>
<li>Log odds of the event as a linear function: <span class="math display">\[log\Big(\frac{p}{1 - p}\Big) = \beta_0 + \beta_1x_1 + \dots + \beta_Px_P\]</span></li>
<li>Event probability: <span class="math display">\[p = \frac{1}{1 + exp[-(\beta_0 + \beta_1x_1 + \dots + \beta_Px_P)]}\]</span></li>
<li>Using maximum likelihood to fit a logistic regression model (given the data and given our choice of model, what value of the parameters of that model make the observed data most likely?): <span class="math display">\[L(\beta) = \prod_{i=1}^nPr(Y = y_i|X = x_i) = \prod_{i=1}^np_i^{y_i}(1 - p_i)^{1-y_i}\]</span> <span class="math display">\[l(\beta) = \sum_{i=1}^n[y_i\text{log}p(x_i) + (1 - y_i)\text{log}(1 - p(x_i))]\]</span></li>
</ul>
</div>
<div id="linear-discriminate-analysis" class="section level4">
<h4>Linear discriminate analysis</h4>
<p>Given Bayes’ rule, <span class="math display">\[Pr[Y = C_l|X] = \frac{Pr[Y = C_l] \cdot Pr[X|Y = C_l]}{Pr[X]}
          = \frac{Pr[Y = C_l] \cdot Pr[X|Y = C_l]}{\sum_{l=1}^{C}Pr[Y = C_l] \cdot Pr[X|Y = C_l]}\]</span> <span class="math inline">\(X\)</span> is classified into group <span class="math inline">\(C_l\)</span> if <span class="math inline">\(Pr[Y = C_l] \cdot Pr[X|Y = C_l]\)</span> has the largest value of all C classes.</p>
<p>Assuming the distribution of the predictors is multivariate normal with a multidimensional mean vector <span class="math inline">\(\mathbf{\mu}_l\)</span> and covariance matrix <span class="math inline">\(\Sigma_l\)</span>, <span class="math display">\[\text{log}(Pr[X|Y = C_l]) = \text{log} f_l(x)
                            = \text{log} \Big(\frac{1}{(2\pi)^\frac{p}{2}|\Sigma_l|^\frac{1}{2}}exp(-\frac{1}{2}(x - \mu_l)^T\Sigma_l^{-1}(x - \mu_l)) \Big)\]</span></p>
<p>Further assuming the covariance matrices are identical across groups, the above equation is equivalent to <span class="math display">\[x^T\Sigma^{-1}\mu_l - \frac{1}{2}\mu_l^T\Sigma^{-1}\mu_l\]</span> Adding the prior probability, we have <span class="math display">\[x^T\Sigma^{-1}\mu_l - \frac{1}{2}\mu_l^T\Sigma^{-1}\mu_l + \text{log}(Pr[Y = C_l])\]</span> Without the assumption of constant covariance, the equation is no longer linear in <span class="math inline">\(X\)</span> and is hence QDA.</p>
<p>LDA advantages over logistic regression:</p>
<ul>
<li>When the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.</li>
<li>If <span class="math inline">\(n\)</span> is small and the distribution of the predictors <span class="math inline">\(X\)</span> is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.</li>
<li>Linear discriminant analysis is popular when we have more than two response classes.</li>
</ul>
<p>LDA limitations:</p>
<ul>
<li>The LDA solution depends on inverting a covariance matrix - a unique solution exists only when the data contain more samples than predictors and the predictors are independent (just like in regression).</li>
<li>Assumption of normal distribution and equal covariance.</li>
</ul>
<p>Extensions to LDA:</p>
<ul>
<li>QDA</li>
<li>RDA: <span class="math display">\[\tilde{\Sigma_l}(\lambda) = \lambda\Sigma_l + (1 - \lambda)\Sigma\]</span> where <span class="math inline">\(\Sigma_l\)</span> is the covariance matrix of the <span class="math inline">\(l\)</span>th class and <span class="math inline">\(\Sigma\)</span> is the pooled covariance matrix across all classes.</li>
<li>MDA: allows each class to be represented by <em>multiple</em> multivariate normal distribution. These distributions can have different means but, like LDA, the covariance structures are assumed to be the same.</li>
</ul>
</div>
<div id="partial-least-squares-discriminant-analysis-plsda" class="section level4">
<h4>Partial least squares discriminant analysis (PLSDA)</h4>
<ul>
<li>PLSDA seeks to find optimal group separation while being guided by between-groups covariance matrix whereas PCA seeks to reduce dimension using the total variation as directed by the overall covariance matrix of the predictors.</li>
<li>However, if dimension reduction is not necessary and classification is the goal, LDA will always provide a lower misclassification rate than PLS.</li>
</ul>
</div>
<div id="penalized-models" class="section level4">
<h4>Penalized models</h4>
<ul>
<li><code>glmnet</code> uses ridge and lasso penalties simultaneously, like the elastic net, but structures the penalty slightly differently: <span class="math display">\[\text{log}L(p) - \lambda\Big[(1 - \alpha)\frac{1}{2}\sum_{j=1}^{P}\beta_j^2 + \alpha\sum_{j=1}^{P}|\beta_j|\Big]\]</span> where <span class="math inline">\(\text{log}L(p)\)</span> is the binomial or multinomial log likelihood.</li>
</ul>
</div>
</div>
<div id="latent-dirichlet-allocation" class="section level3">
<h3>Latent Dirichlet allocation</h3>
<ul>
<li>The intuition behind LDA is that documents exhibit multiple topics.</li>
<li>Document generative process:
<ul>
<li>Randomly choose a distribution over topics. A topic is defined to be a distribution over a fixed vocabulary and is assumed to be specified before any data has been generated.</li>
<li>For each document in the collection, we generate the words in a two-stage process:
<ol style="list-style-type: decimal">
<li>Randomly choose a distribution over topics (i.e., topic profortion).</li>
<li>For each word in the document
<ul>
<li>Randomly choose a topic from the distribution over topics in step #1.</li>
<li>Randomly choose a word from the corresponding distribution over the vocabulary.</li>
</ul></li>
</ol></li>
</ul></li>
<li>This generative process defines a <em>joint probability distribution</em> over both the observed and hidden random variables. We perform data analysis by using that joint distribution to compute the <em>conditional distribution</em> of the hidden variables given the observed variables. This conditional distribution is also called the <em>posterior distribution</em>.
<ul>
<li>The observed variables are the words of the documents</li>
<li>The hidden variables are the topic structure (including the topics, per-document topic distributions, and the per-document per-word topic assignments), and the generative process is as described above. <span class="math display">\[p(\beta_{1:K}, \theta_{1:D}, z_{1:D}|w_{1:D}) = \frac{p(\beta_{1:K}, \theta_{1:D}, z_{1:D}, w_{1:D})}{p(w_{1:D})}\]</span></li>
<li><span class="math inline">\(p(w_{1:D})\)</span> is the probability of seeing the observed corpus under any topic model and, in theory, can be computed by summing the joint distribution over every possible instantiation of the hidden topic structure. Hence, it is intractable to compute.</li>
<li>Topic modeling algorithms form an approximation by forming an alternative distribution over the latent topic structure that is adapted to be close to the true posterior.
<ul>
<li>Sampling based algorithms (e.g., <em>Gibbs sampling</em>) attempt to collect samples from the posterior to approximate it with an empirical distribution.
<ul>
<li>Go through each document <span class="math inline">\(d\)</span>, and randomly assign each word in the document to one of the <span class="math inline">\(K\)</span> topics.</li>
<li>To improve, go through the corpus again, and</li>
<li>for each document <span class="math inline">\(d\)</span>, calculate <span class="math inline">\(p(topic_k|document_d)\)</span>, the proportion of words in document d that are currently assigned to topic <span class="math inline">\(k\)</span>, i.e., the prevelance of the topic.</li>
<li>for all documents, calculate <span class="math inline">\(p(word_d|topic_k)\)</span>, the proportion of assignments to topic <span class="math inline">\(k\)</span> over all documents that come from this word <span class="math inline">\(w\)</span>.</li>
<li>Then reassign <span class="math inline">\(w\)</span> to a new topic <span class="math inline">\(k&#39;\)</span> with the probability <span class="math inline">\(p(topic_{k&#39;}|document_d) \times p(word_d|topic_{k&#39;})\)</span>. In other words, in this step, we’re assuming that all topic assignments except for the current word in question are correct, and then updating the assignment of the current word using our model of how documents are generated.</li>
<li>Repeat until converge.</li>
</ul></li>
<li>Variational methods, rather than approximating the posterior with samples, posit a parameterized family of distributions over the hidden structure and then find the member of that family that is closest to the posterior. Thus, the inference problem is transformed to an optimization problem.</li>
</ul></li>
</ul></li>
</ul>
</div>
