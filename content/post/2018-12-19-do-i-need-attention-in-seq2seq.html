---
title: Do I really need attention in my seq2seq model?
author: Runze
date: '2018-12-17'
slug: do-i-really-need-attention
categories:
  - Data Analysis
tags:
  - Deep Learning
  - NLP
  - RNN
description: ''
topics: []
---

<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/jquery/jquery.min.js"></script>
<link href="/rmarkdown-libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
<script src="/rmarkdown-libs/datatables-binding/datatables.js"></script>
<link href="/rmarkdown-libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="/rmarkdown-libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="/rmarkdown-libs/dt-core/js/jquery.dataTables.min.js"></script>
<link href="/rmarkdown-libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="/rmarkdown-libs/crosstalk/js/crosstalk.min.js"></script>


<div id="background" class="section level3">
<h3>Background</h3>
<p>Since the origin of the idea of the attention mechanism (Bahdanau et al., 2015), it has become a norm to try to add it in a seq2seq model, especially in translations. It is such an intuitive and powerful idea (not to mention the added benefit of peaking into a blackbox model by visualizing its attention weights) that many tutorials and blog posts made it sound like one should not even bother with a simpler model without it as the results would for sure be inferior. (Truthfully, even working on my own <a href="https://runze.github.io/2017/09/07/second-attempt-at-building-a-language-translator/">post</a> on this topic earlier, I completely skipped the simpler approach too.) However, is it really the case? Does the added complexity introduced by an extra attention layer really translate into a significantly superior performance? In this post, I set off to construct a controlled experiment to estimate the exact benefit of using attentions in machine translations.</p>
<p>Before going into the details of the case study, it is worth emphasizing that there is really no general answer or silver bullet to the above question as there are just too many factors that can change the results (e.g., the dataset, the model architecture, just to name a few). Hence, such questions should always be evaluated on a case-by-case basis and the goal of this post is by no means to offer any general advice. Rather, the goal is to show the importance of experimentation.</p>
<p>In the following sections, I‚Äôll describe the key components of the models used in the experiment and present the results in the end. My complete code for this project is hosted <a href="https://github.com/Runze/seq2seq-translation-attention/blob/master/translate-keras.ipynb">here</a>.</p>
</div>
<div id="data" class="section level3">
<h3>Data</h3>
<p>Ideally, to make the study useful, I should run the experiment on a variety of datasets and report the results for each of them. However, due to time constraint, I have only tested it on one parallel English-French dataset provided by the <a href="http://www.manythings.org/anki/">Tatoeba Project</a>. The dataset originally have 155K bilingual pairs. After limiting them to sentences of length 2-20 tokens (by removing less than 1% of the data), I ended up with 154K pairs, of which, 80% were used as training data and the remaining 20% as validation. Keeping all tokens in the training data, I have an English vocabulary of 13K tokens and a French one of 22K. The tokenizations were done using the English and French tokenizers as provided by <a href="https://spacy.io/">spaCy</a>.</p>
</div>
<div id="models" class="section level3">
<h3>Models</h3>
<p>To make sure the results are comparable, I created two models with the exact same architecture with the only exception being an extra attention layer in one of them. Specifically,</p>
<ul>
<li><p>The encoders in both cases are exactly the same (although trained separately). Simply put, each embedded input token from the source language (i.e., English) is processed by a single-layer, unidirectional LSTM with a hidden size of 256.</p></li>
<li><p>In the model without attention, the decoder is just another single-layer, unidirectional LSTM (also of size 256) that takes its initial states from the encoder final states and, in a teacher-forcing fashion, takes its input from the embedded true target language (i.e., French).</p></li>
<li><p>In the model with attention, at each decoding step, the decoder consults an extra attention layer that computes a weighted average of the encoder outputs at all timesteps (with the weight representing the amount of ‚Äúattention‚Äù the decoder should pay to each of them, respectively) and combines that with the current decoder input to generate a prediction. Theoretically, in doing so, instead of cramming all encoded information into the initial states, the decoder has access to all of them at all times, <em>and</em> in an intelligent fashion. In particular, the attention layer I implemented in this experiment is exactly the same as the one I described in my prior <a href="https://runze.github.io/2017/09/07/second-attempt-at-building-a-language-translator/#attention-layer">post</a>.</p></li>
</ul>
<p>Both the source‚Äôs and the target‚Äôs embedding layers are initialized by <a href="https://fasttext.cc/">the fasttext embeddings</a> in the respective languages. Based on my experiment, using these pre-trained embeddings doesn‚Äôt do much to decrease the validation loss but makes it easy for the attention layer to find the right place to attend (based on the visualized attention weights). In terms of model size, both models have about 17M parameters, of which 60% come from the two embedding layers. The one with attention has 100K more parameters.</p>
</div>
<div id="training" class="section level3">
<h3>Training</h3>
<p>Both models were trained using the Adam optimizer and early stopping that was monitored on the validation cross-entropy loss. For the model without attention, it took 8 epochs to see the validation loss plateau; for the case with attention, it took 12. For each epoch on average, the latter also took almost twice as long to train.</p>
</div>
<div id="results" class="section level3">
<h3>Results</h3>
<p>The results from the two models are first evaluated quantitatively using the BLEU score, which essentially counts the amount of overlaps of n-grams between the actual and predicted translations. The results, measured using 4-grams, are shown below (on a scale of 0-1):<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<div id="htmlwidget-1" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"filter":"none","data":[["1","2"],["No","Yes"],[0.69,0.685]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>With attention<\/th>\n      <th>BLEU<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"t","columnDefs":[{"className":"dt-center","targets":1},{"className":"dt-right","targets":2},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>Interestingly, it seems that the model with attention actually resulted in a slightly worse BLEU score in aggregate. But given the benefit of attention, did it do better with the long sentences? To test the hypothesis, I grouped the sentence pairs by the source length and re-computed the score per group below:</p>
<p><img src="https://raw.githubusercontent.com/Runze/seq2seq-translation-attention/master/illustrations/bleu_by_len.png" alt="alt text" width="500"></p>
<p>Based on the plot above, it does seem like the score from the attention model catches up in the end, but still not significantly higher, which is rather disappointing.</p>
<p>But is BLEU really a good metric? Yes, it‚Äôs useful as a scalable quantitative measure but n-gram overlaps don‚Äôt always translate directly into translation quality. To get a better sense, it is often necessary to review some of the translations manually, and this is where <a href="https://twitter.com/jadorelacouture/status/912175822613524480">my expert French skills</a> come to shine! In particular, I reviewed 100 randomly sampled predictions from the two models and compared them with the source and the actual target to determine whether they are good or not. In the table below, I present my results in the last two columns (1 if a translation is considered good):</p>
<div id="htmlwidget-2" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-2">{"x":{"filter":"none","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100"],["whatever happened to your pride ?","i feel kind of tired .","i think that 's tom over there .","tom did n't talk about it .","i just want to be clear .","the wedding will be held in a 17th century church .","are you happy with that ?","he is used to talking to foreigners .","i usually have a light breakfast .","he was injured in the accident .","pizza is the kind of food that fits into today 's life style .","this dictionary has 12 volumes .","at least they listened to me .","that 's not a bad thing .","he told me that he would start the next day .","i heard a woman scream .","i needed your help on something , but i could n't find you .","she believes her son is still alive .","where do you usually go to get a haircut ?","i have responsibilities .","strive to be happy .","i can buy one .","you 've got a fever .","your answer does n't make sense .","he earns three times more than i do .","i want facts .","i had a really great time with your family .","tell whoever comes that i 'm out .","who 's your date tonight ?","our cow does n't give us any milk .","tom was mary 's first love .","try us again next monday .","you really expressed yourself quite clearly .","we 'll take a look at it .","i 'm waiting for a letter from tom .","she belongs to the tennis club .","i seldom hear from him .","what 's the spiciest thing you 've ever eaten ?","a woman appeared from behind a tree .","i am not getting involved .","i 'm at the prison .","i want to take you to dinner .","can we forget that this just happened ?","trade helps nations develop .","we 're almost there .","they should go , regardless of whether they 're men or women .","have you ever been arrested ?","you have to memorize this sentence .","if all goes to plan , i should be back home again tomorrow night .","you must not make noises in the classroom .","i 'm voting no .","i wash my hair every day .","i often recall my happy childhood memories .","with his new job , he 's taken on more responsibility .","the victim is thought to have taken a large quantity of poison by mistake .","our success was due to his efforts .","i suppose you know all about it .","i thought it would be fun .","he is willing enough .","does this bus go to the beach ?","my brother speaks very fast .","is this a tax - free shop ?","can we really learn to speak a foreign language like a native ?","do n't use too much water .","he was fired by the school .","you must be cautious .","the police accused him .","keep away from the dog .","we have an open relationship .","your criticism is unfair .","she was poor , but she was honest .","staying home is n't fun .","stop following me .","could you take some pictures of us ?","his job is to teach english .","i 'm more than a friend .","i wish you 'd stop doing that .","i certainly have had great luck .","it 's going to be great and exciting to be there .","do you have any plans for tomorrow ?","it 's really cool here .","that 's what you all say .","i got hot .","his book inspired me .","i 'm not sure i agree .","i require your assistance .","it is paris that i want to visit .","do n't try to stop me .","i still do n't understand how to do that .","where are you heading ?","she advised him to see the dentist .","i 'd like to discuss the possibility of you coming to work for our company .","i like being on my own .","if you could do it , would you do it ?","we 're both fine .","i wo n't let you down .","i really think i should drive .","can you pass me the salt ?","i do n't care if you 're busy . please help me now .","do you have a match ?"],["qu' est - il advenu de votre fiert√© ?","je me sens comme fatigu√© .","je pense que c' est tom l√†-bas .","tom n' en a pas parl√© .","je veux √™tre claire , un point c' est tout .","le mariage aura lieu dans une √©glise du xviie si√®cle .","en es - tu contente ?","il a l' habitude de parler √† des √©trangers .","je prends habituellement un petit-d√©jeuner l√©ger .","il a √©t√© bless√© durant l' accident .","la pizza est le genre d' alimentation qui convient au style de vie d' aujourd' hui .","ce dictionnaire comporte douze volumes .","au moins , elles m' ont √©cout√©e .","ce n' est pas une mauvaise chose .","il me dit qu' il commencerait le jour suivant .","j' ai entendu crier une femme .","j' ai eu besoin de votre aide mais je n' ai pas pu vous trouver .","elle croit que son fils est encore vivant .","o√π vous rendez-vous habituellement pour vous faire couper les cheveux ?","j' ai des responsabilit√©s .","efforcez - vous d' √™tre heureuses .","je peux en acheter un .","tu as de la fi√®vre .","ta r√©ponse n' a aucun sens .","il gagne trois fois plus que moi .","il me faut des faits .","j' ai vraiment pass√© du bon temps avec ta famille .","dis √† quiconque se pr√©sente que je suis sorti !","qui est ton rancard ce soir ?","notre vache ne nous donne pas de lait .","tom fut le premier amour de mary .","r√©essayez lundi prochain .","tu t' es vraiment exprim√© clairement .","nous y jetterons un ≈ìil .","j' attends une lettre de tom .","elle appartient au club de tennis .","j' entends rarement parler de lui .","quelle est la chose la plus √©pic√©e que vous ayez jamais mang√©e ?","une femme est apparue de derri√®re un arbre .","je ne vais pas m' impliquer .","je suis √† la prison .","je veux vous emmener d√©jeuner .","pouvons - nous oublier que √ßa vient d' arriver ?","le commerce aide les pays √† se d√©velopper .","nous y sommes bient√¥t .","ils devraient y aller , sans consid√©ration pour leur genre .","avez - vous jamais √©t√© arr√™t√©s ?","tu dois m√©moriser cette sentence .","si tout se passe selon le plan , je devrais √™tre de retour √† la maison demain soir .","tu ne dois pas faire de bruit en classe .","je vote non .","je me lave les cheveux quotidiennement .","je me rem√©more souvent mes heureux souvenirs d' enfance .","il a pris davantage de responsabilit√©s avec son nouvel emploi .","on suppose que la victime a absorb√© par erreur une grande quantit√© de poison .","notre r√©ussite fut gr√¢ce √† ses efforts .","je suppose que vous savez tout √† ce sujet .","je pensais que ce serait marrant .","il a suffisamment de volont√© .","est -ce que ce bus va √† la plage ?","mon fr√®re parle tr√®s vite .","est -ce un magasin d√©tax√© ?","peut - on vraiment apprendre √† parler une langue comme un locuteur natif ?","n' utilisez pas trop d' eau .","il a √©t√© expuls√© par l' √©cole .","tu dois √™tre prudent .","la police le mit en cause .","reste √©loign√© du chien .","nous avons des relations ouvertes .","votre critique est injuste .","elle √©tait pauvre , mais elle √©tait honn√™te .","rester chez soi n' a rien d' amusant .","arr√™tez de me suivre .","pourriez - vous prendre quelques photos de nous ¬† ?","son travail consiste √† enseigner l' anglais .","je suis plus qu' une amie .","j' aimerais que tu arr√™tes de faire √ßa .","j' ai certainement eu beaucoup de chance .","√ßa va √™tre super et passionnant d' y √™tre .","as - tu des projets pour demain ?","c' est vraiment d√©tendu , ici .","c' est ce que vous dites tous .","je me suis mise √† avoir chaud .","son livre m' inspira .","je ne suis pas s√ªre d' √™tre d' accord .","je requiers votre assistance .","c' est paris que je veux visiter .","n' essayez pas de m' arr√™ter !","je ne comprends toujours pas comment faire √ßa .","o√π vous dirigez - vous ?","elle lui conseilla de voir le dentiste .","j' aimerais m' entretenir de la possibilit√© que tu viennes travailler pour notre soci√©t√© .","j' appr√©cie d' √™tre seule .","le pourrais - tu , le ferais - tu ?","nous allons bien toutes les deux .","je ne te laisserai pas tomber .","je pense vraiment que je devrais conduire .","peux - tu me passer le sel ?","√ßa m' est √©gal que vous soyez occup√©es . je vous prie de m' aider maintenant .","as - tu une allumette ?"],["qu' est -ce qui t' a eu lieu ?","je me sens fatigu√© , un point de temps .","je pense que c' est tom l√†-bas .","tom n' en a pas parl√© .","je veux simplement √™tre clair .","le mariage se trouvait sur la plage avant d' aller .","en √™tes - vous contentes ?","il est habitu√© √† parler d' √©trangers .","j' ai pris un petit d√©jeuner l√©ger .","il a √©t√© bless√© dans l' accident .","la pizza est la pizza , c' est de l' autre livre de la cuisine aujourd'hui .","ce dictionnaire a besoin d' un complet .","au moins , ils m' ont √©cout√© .","ce n' est pas une mauvaise chose .","il m' a dit qu' il allait se passer du jour suivant .","j' ai entendu un homme crier .","j' ai eu besoin de toi de quelque chose qui aurait pu m' aider .","elle pense que son fils est toujours en vie .","o√π te rends - tu habituellement pour te faire couper les cheveux ?","j' ai des responsabilit√©s .","efforce - toi d' √™tre heureuse .","je peux en acheter un .","tu as de la fi√®vre .","ta r√©ponse n' est pas vraiment .","il est plus trois fois plus que je l' en ai .","je veux des faits .","j' ai pass√© un super bon boulot .","dites √† ma femme que le point est .","qui est ton d√Æner avec ce soir ?","notre soupe ne peut pas donner du lait .","tom √©tait le premier √† l' amour de marie .","fais - nous encore de nouveau lundi prochain !","tu t' es tr√®s bien fait toi-m√™me .","nous allons jeter un ≈ìil l√†-dessus .","j' attends une lettre de tom .","elle a jou√© au tennis dans le prix .","je les ai rarement de l' entendre .","quelle est la chose la plus √©pic√©e que tu aies jamais mang√©e ?","une femme a perdu une ferme d' arbre .","je ne me suis pas impliqu√© .","je suis en prison .","je veux te prendre d√Æner .","pouvons - nous avoir appel√© pour ce qu' il n' est pas arriv√© ?","le conseil a besoin d' √©lectricit√© .","nous y sommes presque .","ils devraient y faire des hommes mais ils font les hommes que les hommes sont sans importance .","as - tu jamais √©t√© arr√™t√©e ?","tu dois m√©moriser cette sentence .","bien que le moment , je devrais aller chez moi , demain soir .","tu ne dois pas faire les cours dans les filles .","je ne fais pas .","je me lave les cheveux tous les jours .","je me souviens me rappelle tous les deux amis √† mon secours .","√† nouveau avec son travail √† nouveau il a √©t√© plus √©lev√© .","la mani√®re dont il n' a pas eu de l' ombre √† avoir de l' homme temporaire .","son succ√®s a √©t√© d√ª aux √©lections .","je suppose que tu sais tout √† fait .","je pensais que √ßa serait amusant .","il est assez s√ªr .","est -ce que cela peut aller √† la plage pour le pied ?","mon fr√®re parle tr√®s vite .","est -ce un magasin de libre les jeux olympiques ?","pouvons - nous apprendre une telle apprendre une langue √©trang√®re ?","n' utilise pas trop de l' eau .","il a √©t√© renvoy√© en l' √©cole .","il faut que vous soyez prudents .","la police l' a accus√© .","restez √† distance du chien .","nous avons une relation inhabituelle .","ta critique est injuste .","elle √©tait pauvre mais elle √©tait honn√™te .","rester √† la maison est amusant .","arr√™te de me taquiner !","peux - tu prendre quelques photos de nous en ?","son travail est l' anglais √† l' apprentissage .","je suis plus qu' un ami .","j' aimerais que tu arr√™tes de faire √ßa .","je serais certainement en chance , devine .","il va √™tre tr√®s amusant et le temps .","as - tu des projets pour demain ?","c' est vraiment frais , ici .","c' est ce que vous avez tout dit .","j' ai chaud .","mon livre a cess√© de rire .","je ne suis pas s√ªre d' √™tre s√ªr .","j' ai besoin de votre assistance .","il est all√© √† paris pour visiter ma famille .","n' essaie pas de m' arr√™ter !","je n' ai pas toujours √† faire √ßa √† faire .","o√π vas - tu te taire ?","elle lui conseilla de voir le dentiste .","j' aimerais en discuter de la possibilit√© que tu viennes travailler avec notre entreprise .","j' aime √™tre seul .","si tu le ferais , tu le ferais ?","nous nous sommes tous deux .","je ne te laisserai pas tomber .","je pense vraiment que je devrais conduire .","pouvez - vous me passer le sel ?","√ßa m' est √©gal que tu sois occup√©e . je te prie , maintenant aider .","avez - vous une allumette ?"],["qu' est - il advenu de ma part de la fiert√© ?","je me sens assez fatigu√©e .","je pense que c' est tom ici .","tom n' a pas parl√© .","je veux tout simplement √™tre clair .","le mariage saura se baigner dans un club de cuisine .","en √™tes - vous contents ?","il est habitu√© √† parler .","j' ai sommeil un repas l√©ger .","il a √©t√© bless√© √† l' accident .","la nourriture est diff√©rente de la profondeur de ce paragraphe est diff√©rente de la saint pr√©f√©r√©e .","ce dictionnaire compte 12 volumes .","au moins , m' entends - elle !","ce n' est pas une mauvaise chose .","il m' a dit qu' il allait le voir lundi .","j' ai entendu une femme tu√© .","j' ai eu besoin de votre aide , mais je ne pense pas que je vous aie dit .","elle croit tout le monde qu' elle est mort .","o√π vas - tu habituellement si tu me fasse visiter ?","je me fais marrer .","efforcez - vous d' √™tre heureux .","je peux en acheter une .","tu as de la fi√®vre .","votre r√©ponse n' a pas de sens .","il gagne trois fois plus que moi .","je veux des faits .","j' ai fait un tr√®s bon temps √† ta famille .","dis - le que je suis en train de sortir .","qui est votre amie , ce soir ?","notre vache ne veut pas encore lait .","tom √©tait l' amour de foudre .","nous allons encore jusqu' √† lundi prochain .","tu as vraiment bien √©t√© s√©rieux .","nous y allons jeter un ≈ìil .","j' attends une lettre d' ici .","elle est au bureau de tennis en conserve .","il m' a ri de lui .","quelle est la chose la plus √©pic√©e que tu aies jamais mang√©e ?","une femme est apparu dans un arbre .","je ne suis pas impliqu√© .","je suis en prison .","je veux vous emmener d√Æner .","pouvons - nous d√©terminer ce qui est arriv√© ?","l' aide d' √©trangers laisse les √©trangers .","nous y sommes presque .","ils devraient √™tre en col√®re et ils font les femmes , encore trop femmes .","avez - vous jamais √©t√© arr√™t√©es ?","tu dois m√©moriser cette phrase .","si tout va bien chercher , je devrais √™tre silencieux demain .","tu ne dois pas faire des r√®gles dans la salle .","je ne sais pas .","je lave mes cheveux tous les jours .","je me suis souvent nerveux √† mon enfance si heureuse .","√† son poste , son aide , il est occup√© de la prison .","la victime est inconnue en plus √©trange , la mauvaise r√©colte de mauvaises herbes .","notre mariage a √©t√© difficile de ses d√©fauts .","je suppose que tu sais tout √† √ßa .","je pensais que ce serait amusant .","il est tout en fait assez .","cet h√¥tel est-il √† la plage ?","mon fr√®re parle tr√®s vite .","est -ce une imitation ?","savez-vous apprendre √† parler beaucoup d' argent en une banane ?","n' utilisez pas trop d' eau .","il a √©t√© renvoy√© √† l' √©cole .","il vous faut √™tre prudente .","la police l' a accus√© .","restez √©loign√©es du chien .","nous avons une atmosph√®re accueillante .","ta critique est injuste .","elle √©tait pauvre mais elle a raison .","rester chez soi n' est pas marrant .","ne me mets pas le courrier .","pourrais - tu nous prendre de photos ?","son livre est d' enseigner √† l' anglais .","je suis plus un ami .","j' aimerais que tu arr√™tes de faire √ßa .","j' ai d√ª bien la sant√© .","√ßa va √™tre bient√¥t pour √™tre haut .","avez - vous des boissons que vous soyez venus ?","c' est vraiment relaxant , ici .","c' est ce que vous dites tout .","j' ai chaud .","son livre me rend - il .","je ne suis pas s√ªre d' √™tre honn√™te .","j' ai besoin de ton assistance .","il est probable que je veux visiter l' universit√© .","n' essayez pas de m' arr√™ter !","je ne comprends toujours pas cela .","o√π vous mettez - vous ?","elle lui a conseill√© de voir le dentiste .","j' aimerais m' entretenir de la possibilit√© que tu viennes travailler pour notre soci√©t√© .","j' aime me trouver seul .","si tu pouvais le faire , √ßa le fera ?","nous nous amusons tous les deux .","je ne vous laisserai pas tomber .","je pense vraiment que je devrais conduire .","pouvez - vous me passer le sel ?","√ßa m' est √©gal que vous soyez occup√©e . je vous prie de m' aider maintenant .","avez - vous une allumette ?"],[null,null,1,1,1,null,1,null,null,1,null,null,1,1,null,null,null,1,1,1,1,1,1,null,null,1,null,null,null,null,1,null,null,1,1,null,null,1,null,null,1,null,null,null,1,null,1,1,null,null,null,1,null,null,null,null,null,1,null,null,1,null,null,1,null,1,1,1,null,1,1,null,null,null,null,1,1,null,null,1,1,1,1,null,null,1,null,1,null,null,1,null,1,null,null,1,1,1,null,1],[null,1,null,null,1,null,1,null,null,null,null,1,null,1,null,null,null,null,null,null,1,1,1,1,1,1,null,null,null,null,null,null,null,1,null,null,null,1,null,null,1,1,null,null,1,null,1,1,null,null,null,null,null,null,null,null,1,1,null,null,1,null,null,1,null,1,1,1,null,1,null,1,null,null,null,null,1,null,null,null,1,1,1,null,null,1,null,1,null,null,1,1,1,null,null,1,1,1,1,1]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>source<\/th>\n      <th>true_target<\/th>\n      <th>pred_wo_attn<\/th>\n      <th>pred_w_attn<\/th>\n      <th>good_wo_attn<\/th>\n      <th>good_w_attn<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":[5,6]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>In aggregate, the predictions from the no-attention model are deemed good 45% of the time and those from the attention model are good 41%. The difference is not statistically significant but it at least shows that the added attention layer did not add any significant improvement to the translation quality.</p>
<p>While reviewing these translations myself, I have also noticed the notorious ‚Äú<a href="https://arxiv.org/abs/1809.02156">hallucination</a>‚Äù problem that plagues the image caption models where, in my case, the model generates pieces of translations that are not present in the source. For example, in the 6th example above, the source sentence is ‚Äúthe wedding will be held in a 17th century church,‚Äù but neither predictions get the location of the wedding correct (the first one says on the beach and the second says in a cooking club).<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> The phenomenon is likely due to bias in the language priors (the decoder is a language model after all), which makes me a bit worried about whether those pre-trained language models, despite having been shown to significantly improve many downstream tasks, can potentially exacerbate this particular problem.</p>
<p>Comparisons aside, I was also curious to see whether the learned attention weights could show that the decoder was at least paying attention to the ‚Äúright‚Äù places from the encoder outputs. To get an idea visually, I randomly sampled 9 translations that match the true target exactly and visualized their attention weights below. (The horizontal axis is the source and the vertical is the target. The whiter a cell is, the heavier its attention weight is.) After seeing the results, I can only say‚Ä¶ well, kind of.</p>
<p><img src="https://raw.githubusercontent.com/Runze/seq2seq-translation-attention/master/illustrations/attn_weights.png" alt="alt text" width="800"></p>
<p>First of all, as shown above, unlike the crisp, precise patterns shown in the original paper, my attention weights are often spreaded out horizontally, meaning the decoder did not have a good idea where to look at exactly. There are some accurate cases (e.g., in the second matrix, <em>gar√ßon</em> is mapped perfectly to <em>boy</em>, and in the 4th one, <em>pense</em> is mapped to <em>think</em>), but they are generally rather rare. Secondly, the attention weights usually fade out as the decoding timestep increases (i.e., as we move up vertically in the matrix). This is worrisome because it implies that the translations at the later timesteps received little help from the attention layer, which is ironically what the layer is for in the first place. Hence, all things considered, despite increasing the model size and making the training process longer, the attention layer in this case was still under-trained and did not offer too much help in the end.üòî</p>
</div>
<div id="conclusion" class="section level3">
<h3>Conclusion</h3>
<p>In this project, I tried to estimate the effect of the attention layer in a seq2seq translation model by performing a simple controlled experiment. After having compared the BLEU scores, manually reviewed a sample of the results, and visualized the learned attention weights, I unfortunately did not find any evidence suggesting any added benefit from the extra attention layer.</p>
<p>That said, as mentioned in the beginning of this post, a <em>big</em> caveat of this study is that the above results are based on this particular dataset and these particular model architectures only. For example, the way I implemented the attention layer is only one of the many ways of doing so and there are <em>many</em> other approaches out there ranging from simple dot products between the previous hidden state and each of the encoder outputs to this rather peculiar way of only using the previous hidden state and the current decoder input as implemented in this PyTorch <a href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#attention-decoder">tutorial</a>.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> Hence, without experimenting with any alternative architectures (or tuning the various hyperparameters), the goal of this project is not at all to say an attention layer is not necessary. Rather, my goal is simply to urge practitioners to always build a <em>solid</em>, albeit boring, baseline first before venturing into a much more complicated solution. What you find may very well surprise you, as it did here for me.üôÇ</p>
</div>
<div id="references" class="section level3">
<h3>References</h3>
<p>Bahdanau, D., Cho, K., &amp; Bengio, Y. (2015). Neural Machine Translation by Jointly Learning to Align and Translate. In ICLR 2015.</p>
<p>Dichao Hu (2018). An Introductory Survey on Attention Mechanisms in NLP Problems.</p>
<p>Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, Kate Saenko (2018). Object Hallucination in Image Captioning.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>When constructing the references for each predicted translation, I took advantage of the fact that the dataset has multiple target sentence for a single source sentence, so each prediction is evaluated against all the available references.<a href="#fnref1">‚Ü©</a></p></li>
<li id="fn2"><p>Both translations also just don‚Äôt make sense.<a href="#fnref2">‚Ü©</a></p></li>
<li id="fn3"><p>This survey <a href="https://arxiv.org/abs/1811.05544v1">paper</a> is a good starting point to learn about the different common implementations of the attention layers.<a href="#fnref3">‚Ü©</a></p></li>
</ol>
</div>
