---
title: Do I really need attention in my seq2seq model?
author: Runze
date: '2018-12-17'
slug: do-i-really-need-attention
categories:
  - Data Analysis
tags:
  - Deep Learning
  - NLP
  - RNN
description: ''
topics: []
---

<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/jquery/jquery.min.js"></script>
<link href="/rmarkdown-libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
<script src="/rmarkdown-libs/datatables-binding/datatables.js"></script>
<link href="/rmarkdown-libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="/rmarkdown-libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="/rmarkdown-libs/dt-core/js/jquery.dataTables.min.js"></script>
<link href="/rmarkdown-libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="/rmarkdown-libs/crosstalk/js/crosstalk.min.js"></script>


<div id="background" class="section level3">
<h3>Background</h3>
<p>Since the origin of the idea of the attention mechanism (Bahdanau et al., 2015), it has become a norm to try to add it in a seq2seq model, especially in translations. It is such an intuitive and powerful idea (not to mention the added benefit of peaking into a blackbox model by visualizing its attention weights) that many tutorials and blog posts made it sound like one should not even bother with a simpler model without it as the results would for sure be inferior. (Truthfully, even working on my own <a href="https://runze.github.io/2017/09/07/second-attempt-at-building-a-language-translator/">post</a> on this topic earlier, I completely skipped the simpler approach too.) However, is it really the case? Does the added complexity introduced by an extra attention layer really translate into a significantly superior performance? In this post, I set off to construct a controlled experiment to estimate the exact benefit of using attentions in machine translations.</p>
<p>Before going into the details of the case study, it is worth emphasizing that there is really no general answer or silver bullet to the above question as there are just too many factors that can change the results (e.g., the dataset, the model architecture, just to name a few). Hence, such questions should always be evaluated on a case-by-case basis and the goal of this post is by no means to offer any general advice. Rather, the goal is to show the importance of experimentation.</p>
<p>In the following sections, I’ll describe the key components of the models used in the experiment and present the results in the end. My complete code for this project is hosted <a href="https://github.com/Runze/seq2seq-translation-attention/blob/master/translate-keras.ipynb">here</a>.</p>
</div>
<div id="data" class="section level3">
<h3>Data</h3>
<p>Ideally, to make the study useful, I should run the experiment on a variety of datasets and report the results for each of them. However, due to time constraint, I have only tested it on one parallel English-French dataset provided by the <a href="http://www.manythings.org/anki/">Tatoeba Project</a>. The dataset originally have 155K bilingual pairs. After limiting them to sentences of length 2-20 tokens (by removing less than 1% of the data), I ended up with 154K pairs, of which, 80% were used as training data and the remaining 20% as validation. Keeping all tokens in the training data, I have an English vocabulary of 13K tokens and a French one of 22K. The tokenizations were done using the English and French tokenizers as provided by <a href="https://spacy.io/">spaCy</a>.</p>
</div>
<div id="models" class="section level3">
<h3>Models</h3>
<p>To make sure the results are comparable, I created two models with the exact same architecture with the only exception being an extra attention layer in one of them. Specifically,</p>
<ul>
<li><p>The encoders in both cases are exactly the same (although trained separately). Simply put, each embedded input token from the source language (i.e., English) is processed by a single-layer, unidirectional LSTM with a hidden size of 256.</p></li>
<li><p>In the model without attention, the decoder is just another single-layer, unidirectional LSTM (also of size 256) that takes its initial states from the encoder final states and, in a teacher-forcing fashion, takes its input from the embedded true target language (i.e., French).</p></li>
<li><p>In the model with attention, at each decoding step, the decoder consults an extra attention layer that computes a weighted average of the encoder outputs at all timesteps (with the weight representing the amount of “attention” the decoder should pay to each of them, respectively) and combines that with the current decoder input to generate a prediction. Theoretically, in doing so, instead of cramming all encoded information into the initial states, the decoder has access to all of them at all times, <em>and</em> in an intelligent fashion. In particular, the attention layer I implemented in this experiment is exactly the same as the one I described in my prior <a href="https://runze.github.io/2017/09/07/second-attempt-at-building-a-language-translator/#attention-layer">post</a>.</p></li>
</ul>
<p>Both the source’s and the target’s embedding layers are initialized by <a href="https://fasttext.cc/">the fasttext embeddings</a> in the respective languages. Based on my experiment, using these pre-trained embeddings doesn’t do much to decrease the validation loss but makes it easy for the attention layer to find the right place to attend (based on the visualized attention weights). In terms of model size, both models have about 17M parameters, of which 60% come from the two embedding layers. The one with attention has 100K more parameters.</p>
</div>
<div id="training" class="section level3">
<h3>Training</h3>
<p>Both models were trained using the Adam optimizer and early stopping that was monitored on the validation cross-entropy loss. For the model without attention, it took 8 epochs to see the validation loss plateau; for the case with attention, it took 12. For each epoch on average, the latter also took almost twice as long to train.</p>
</div>
<div id="results" class="section level3">
<h3>Results</h3>
<p>The results from the two models are first evaluated quantitatively using the BLEU score, which essentially counts the amount of overlaps of n-grams between the actual and predicted translations. The results, measured using 4-grams, are shown below (on a scale of 0-1):<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<div id="htmlwidget-1" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"filter":"none","data":[["1","2"],["No","Yes"],[0.69,0.685]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>With attention<\/th>\n      <th>BLEU<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"t","columnDefs":[{"className":"dt-center","targets":1},{"className":"dt-right","targets":2},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>Interestingly, it seems that the model with attention actually resulted in a slightly worse BLEU score in aggregate. But given the benefit of attention, did it do better with the long sentences? To test the hypothesis, I grouped the sentence pairs by the source length and re-computed the score per group below:</p>
<p><img src="https://raw.githubusercontent.com/Runze/seq2seq-translation-attention/master/illustrations/bleu_by_len.png" alt="alt text" width="500"></p>
<p>Based on the plot above, it does seem like the score from the attention model catches up in the end, but still not significantly higher, which is rather disappointing.</p>
<p>But is BLEU really a good metric? Yes, it’s useful as a scalable quantitative measure but n-gram overlaps don’t always translate directly into translation quality. To get a better sense, it is often necessary to review some of the translations manually, and this is where <a href="https://twitter.com/jadorelacouture/status/912175822613524480">my expert French skills</a> come to shine! In particular, I reviewed 100 randomly sampled predictions from the two models and compared them with the source and the actual target to determine whether they are good or not. In the table below, I present my results in the last two columns (1 if a translation is considered good):</p>
<div id="htmlwidget-2" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-2">{"x":{"filter":"none","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100"],["whatever happened to your pride ?","i feel kind of tired .","i think that 's tom over there .","tom did n't talk about it .","i just want to be clear .","the wedding will be held in a 17th century church .","are you happy with that ?","he is used to talking to foreigners .","i usually have a light breakfast .","he was injured in the accident .","pizza is the kind of food that fits into today 's life style .","this dictionary has 12 volumes .","at least they listened to me .","that 's not a bad thing .","he told me that he would start the next day .","i heard a woman scream .","i needed your help on something , but i could n't find you .","she believes her son is still alive .","where do you usually go to get a haircut ?","i have responsibilities .","strive to be happy .","i can buy one .","you 've got a fever .","your answer does n't make sense .","he earns three times more than i do .","i want facts .","i had a really great time with your family .","tell whoever comes that i 'm out .","who 's your date tonight ?","our cow does n't give us any milk .","tom was mary 's first love .","try us again next monday .","you really expressed yourself quite clearly .","we 'll take a look at it .","i 'm waiting for a letter from tom .","she belongs to the tennis club .","i seldom hear from him .","what 's the spiciest thing you 've ever eaten ?","a woman appeared from behind a tree .","i am not getting involved .","i 'm at the prison .","i want to take you to dinner .","can we forget that this just happened ?","trade helps nations develop .","we 're almost there .","they should go , regardless of whether they 're men or women .","have you ever been arrested ?","you have to memorize this sentence .","if all goes to plan , i should be back home again tomorrow night .","you must not make noises in the classroom .","i 'm voting no .","i wash my hair every day .","i often recall my happy childhood memories .","with his new job , he 's taken on more responsibility .","the victim is thought to have taken a large quantity of poison by mistake .","our success was due to his efforts .","i suppose you know all about it .","i thought it would be fun .","he is willing enough .","does this bus go to the beach ?","my brother speaks very fast .","is this a tax - free shop ?","can we really learn to speak a foreign language like a native ?","do n't use too much water .","he was fired by the school .","you must be cautious .","the police accused him .","keep away from the dog .","we have an open relationship .","your criticism is unfair .","she was poor , but she was honest .","staying home is n't fun .","stop following me .","could you take some pictures of us ?","his job is to teach english .","i 'm more than a friend .","i wish you 'd stop doing that .","i certainly have had great luck .","it 's going to be great and exciting to be there .","do you have any plans for tomorrow ?","it 's really cool here .","that 's what you all say .","i got hot .","his book inspired me .","i 'm not sure i agree .","i require your assistance .","it is paris that i want to visit .","do n't try to stop me .","i still do n't understand how to do that .","where are you heading ?","she advised him to see the dentist .","i 'd like to discuss the possibility of you coming to work for our company .","i like being on my own .","if you could do it , would you do it ?","we 're both fine .","i wo n't let you down .","i really think i should drive .","can you pass me the salt ?","i do n't care if you 're busy . please help me now .","do you have a match ?"],["qu' est - il advenu de votre fierté ?","je me sens comme fatigué .","je pense que c' est tom là-bas .","tom n' en a pas parlé .","je veux être claire , un point c' est tout .","le mariage aura lieu dans une église du xviie siècle .","en es - tu contente ?","il a l' habitude de parler à des étrangers .","je prends habituellement un petit-déjeuner léger .","il a été blessé durant l' accident .","la pizza est le genre d' alimentation qui convient au style de vie d' aujourd' hui .","ce dictionnaire comporte douze volumes .","au moins , elles m' ont écoutée .","ce n' est pas une mauvaise chose .","il me dit qu' il commencerait le jour suivant .","j' ai entendu crier une femme .","j' ai eu besoin de votre aide mais je n' ai pas pu vous trouver .","elle croit que son fils est encore vivant .","où vous rendez-vous habituellement pour vous faire couper les cheveux ?","j' ai des responsabilités .","efforcez - vous d' être heureuses .","je peux en acheter un .","tu as de la fièvre .","ta réponse n' a aucun sens .","il gagne trois fois plus que moi .","il me faut des faits .","j' ai vraiment passé du bon temps avec ta famille .","dis à quiconque se présente que je suis sorti !","qui est ton rancard ce soir ?","notre vache ne nous donne pas de lait .","tom fut le premier amour de mary .","réessayez lundi prochain .","tu t' es vraiment exprimé clairement .","nous y jetterons un œil .","j' attends une lettre de tom .","elle appartient au club de tennis .","j' entends rarement parler de lui .","quelle est la chose la plus épicée que vous ayez jamais mangée ?","une femme est apparue de derrière un arbre .","je ne vais pas m' impliquer .","je suis à la prison .","je veux vous emmener déjeuner .","pouvons - nous oublier que ça vient d' arriver ?","le commerce aide les pays à se développer .","nous y sommes bientôt .","ils devraient y aller , sans considération pour leur genre .","avez - vous jamais été arrêtés ?","tu dois mémoriser cette sentence .","si tout se passe selon le plan , je devrais être de retour à la maison demain soir .","tu ne dois pas faire de bruit en classe .","je vote non .","je me lave les cheveux quotidiennement .","je me remémore souvent mes heureux souvenirs d' enfance .","il a pris davantage de responsabilités avec son nouvel emploi .","on suppose que la victime a absorbé par erreur une grande quantité de poison .","notre réussite fut grâce à ses efforts .","je suppose que vous savez tout à ce sujet .","je pensais que ce serait marrant .","il a suffisamment de volonté .","est -ce que ce bus va à la plage ?","mon frère parle très vite .","est -ce un magasin détaxé ?","peut - on vraiment apprendre à parler une langue comme un locuteur natif ?","n' utilisez pas trop d' eau .","il a été expulsé par l' école .","tu dois être prudent .","la police le mit en cause .","reste éloigné du chien .","nous avons des relations ouvertes .","votre critique est injuste .","elle était pauvre , mais elle était honnête .","rester chez soi n' a rien d' amusant .","arrêtez de me suivre .","pourriez - vous prendre quelques photos de nous   ?","son travail consiste à enseigner l' anglais .","je suis plus qu' une amie .","j' aimerais que tu arrêtes de faire ça .","j' ai certainement eu beaucoup de chance .","ça va être super et passionnant d' y être .","as - tu des projets pour demain ?","c' est vraiment détendu , ici .","c' est ce que vous dites tous .","je me suis mise à avoir chaud .","son livre m' inspira .","je ne suis pas sûre d' être d' accord .","je requiers votre assistance .","c' est paris que je veux visiter .","n' essayez pas de m' arrêter !","je ne comprends toujours pas comment faire ça .","où vous dirigez - vous ?","elle lui conseilla de voir le dentiste .","j' aimerais m' entretenir de la possibilité que tu viennes travailler pour notre société .","j' apprécie d' être seule .","le pourrais - tu , le ferais - tu ?","nous allons bien toutes les deux .","je ne te laisserai pas tomber .","je pense vraiment que je devrais conduire .","peux - tu me passer le sel ?","ça m' est égal que vous soyez occupées . je vous prie de m' aider maintenant .","as - tu une allumette ?"],["qu' est -ce qui t' a eu lieu ?","je me sens fatigué , un point de temps .","je pense que c' est tom là-bas .","tom n' en a pas parlé .","je veux simplement être clair .","le mariage se trouvait sur la plage avant d' aller .","en êtes - vous contentes ?","il est habitué à parler d' étrangers .","j' ai pris un petit déjeuner léger .","il a été blessé dans l' accident .","la pizza est la pizza , c' est de l' autre livre de la cuisine aujourd'hui .","ce dictionnaire a besoin d' un complet .","au moins , ils m' ont écouté .","ce n' est pas une mauvaise chose .","il m' a dit qu' il allait se passer du jour suivant .","j' ai entendu un homme crier .","j' ai eu besoin de toi de quelque chose qui aurait pu m' aider .","elle pense que son fils est toujours en vie .","où te rends - tu habituellement pour te faire couper les cheveux ?","j' ai des responsabilités .","efforce - toi d' être heureuse .","je peux en acheter un .","tu as de la fièvre .","ta réponse n' est pas vraiment .","il est plus trois fois plus que je l' en ai .","je veux des faits .","j' ai passé un super bon boulot .","dites à ma femme que le point est .","qui est ton dîner avec ce soir ?","notre soupe ne peut pas donner du lait .","tom était le premier à l' amour de marie .","fais - nous encore de nouveau lundi prochain !","tu t' es très bien fait toi-même .","nous allons jeter un œil là-dessus .","j' attends une lettre de tom .","elle a joué au tennis dans le prix .","je les ai rarement de l' entendre .","quelle est la chose la plus épicée que tu aies jamais mangée ?","une femme a perdu une ferme d' arbre .","je ne me suis pas impliqué .","je suis en prison .","je veux te prendre dîner .","pouvons - nous avoir appelé pour ce qu' il n' est pas arrivé ?","le conseil a besoin d' électricité .","nous y sommes presque .","ils devraient y faire des hommes mais ils font les hommes que les hommes sont sans importance .","as - tu jamais été arrêtée ?","tu dois mémoriser cette sentence .","bien que le moment , je devrais aller chez moi , demain soir .","tu ne dois pas faire les cours dans les filles .","je ne fais pas .","je me lave les cheveux tous les jours .","je me souviens me rappelle tous les deux amis à mon secours .","à nouveau avec son travail à nouveau il a été plus élevé .","la manière dont il n' a pas eu de l' ombre à avoir de l' homme temporaire .","son succès a été dû aux élections .","je suppose que tu sais tout à fait .","je pensais que ça serait amusant .","il est assez sûr .","est -ce que cela peut aller à la plage pour le pied ?","mon frère parle très vite .","est -ce un magasin de libre les jeux olympiques ?","pouvons - nous apprendre une telle apprendre une langue étrangère ?","n' utilise pas trop de l' eau .","il a été renvoyé en l' école .","il faut que vous soyez prudents .","la police l' a accusé .","restez à distance du chien .","nous avons une relation inhabituelle .","ta critique est injuste .","elle était pauvre mais elle était honnête .","rester à la maison est amusant .","arrête de me taquiner !","peux - tu prendre quelques photos de nous en ?","son travail est l' anglais à l' apprentissage .","je suis plus qu' un ami .","j' aimerais que tu arrêtes de faire ça .","je serais certainement en chance , devine .","il va être très amusant et le temps .","as - tu des projets pour demain ?","c' est vraiment frais , ici .","c' est ce que vous avez tout dit .","j' ai chaud .","mon livre a cessé de rire .","je ne suis pas sûre d' être sûr .","j' ai besoin de votre assistance .","il est allé à paris pour visiter ma famille .","n' essaie pas de m' arrêter !","je n' ai pas toujours à faire ça à faire .","où vas - tu te taire ?","elle lui conseilla de voir le dentiste .","j' aimerais en discuter de la possibilité que tu viennes travailler avec notre entreprise .","j' aime être seul .","si tu le ferais , tu le ferais ?","nous nous sommes tous deux .","je ne te laisserai pas tomber .","je pense vraiment que je devrais conduire .","pouvez - vous me passer le sel ?","ça m' est égal que tu sois occupée . je te prie , maintenant aider .","avez - vous une allumette ?"],["qu' est - il advenu de ma part de la fierté ?","je me sens assez fatiguée .","je pense que c' est tom ici .","tom n' a pas parlé .","je veux tout simplement être clair .","le mariage saura se baigner dans un club de cuisine .","en êtes - vous contents ?","il est habitué à parler .","j' ai sommeil un repas léger .","il a été blessé à l' accident .","la nourriture est différente de la profondeur de ce paragraphe est différente de la saint préférée .","ce dictionnaire compte 12 volumes .","au moins , m' entends - elle !","ce n' est pas une mauvaise chose .","il m' a dit qu' il allait le voir lundi .","j' ai entendu une femme tué .","j' ai eu besoin de votre aide , mais je ne pense pas que je vous aie dit .","elle croit tout le monde qu' elle est mort .","où vas - tu habituellement si tu me fasse visiter ?","je me fais marrer .","efforcez - vous d' être heureux .","je peux en acheter une .","tu as de la fièvre .","votre réponse n' a pas de sens .","il gagne trois fois plus que moi .","je veux des faits .","j' ai fait un très bon temps à ta famille .","dis - le que je suis en train de sortir .","qui est votre amie , ce soir ?","notre vache ne veut pas encore lait .","tom était l' amour de foudre .","nous allons encore jusqu' à lundi prochain .","tu as vraiment bien été sérieux .","nous y allons jeter un œil .","j' attends une lettre d' ici .","elle est au bureau de tennis en conserve .","il m' a ri de lui .","quelle est la chose la plus épicée que tu aies jamais mangée ?","une femme est apparu dans un arbre .","je ne suis pas impliqué .","je suis en prison .","je veux vous emmener dîner .","pouvons - nous déterminer ce qui est arrivé ?","l' aide d' étrangers laisse les étrangers .","nous y sommes presque .","ils devraient être en colère et ils font les femmes , encore trop femmes .","avez - vous jamais été arrêtées ?","tu dois mémoriser cette phrase .","si tout va bien chercher , je devrais être silencieux demain .","tu ne dois pas faire des règles dans la salle .","je ne sais pas .","je lave mes cheveux tous les jours .","je me suis souvent nerveux à mon enfance si heureuse .","à son poste , son aide , il est occupé de la prison .","la victime est inconnue en plus étrange , la mauvaise récolte de mauvaises herbes .","notre mariage a été difficile de ses défauts .","je suppose que tu sais tout à ça .","je pensais que ce serait amusant .","il est tout en fait assez .","cet hôtel est-il à la plage ?","mon frère parle très vite .","est -ce une imitation ?","savez-vous apprendre à parler beaucoup d' argent en une banane ?","n' utilisez pas trop d' eau .","il a été renvoyé à l' école .","il vous faut être prudente .","la police l' a accusé .","restez éloignées du chien .","nous avons une atmosphère accueillante .","ta critique est injuste .","elle était pauvre mais elle a raison .","rester chez soi n' est pas marrant .","ne me mets pas le courrier .","pourrais - tu nous prendre de photos ?","son livre est d' enseigner à l' anglais .","je suis plus un ami .","j' aimerais que tu arrêtes de faire ça .","j' ai dû bien la santé .","ça va être bientôt pour être haut .","avez - vous des boissons que vous soyez venus ?","c' est vraiment relaxant , ici .","c' est ce que vous dites tout .","j' ai chaud .","son livre me rend - il .","je ne suis pas sûre d' être honnête .","j' ai besoin de ton assistance .","il est probable que je veux visiter l' université .","n' essayez pas de m' arrêter !","je ne comprends toujours pas cela .","où vous mettez - vous ?","elle lui a conseillé de voir le dentiste .","j' aimerais m' entretenir de la possibilité que tu viennes travailler pour notre société .","j' aime me trouver seul .","si tu pouvais le faire , ça le fera ?","nous nous amusons tous les deux .","je ne vous laisserai pas tomber .","je pense vraiment que je devrais conduire .","pouvez - vous me passer le sel ?","ça m' est égal que vous soyez occupée . je vous prie de m' aider maintenant .","avez - vous une allumette ?"],[null,null,1,1,1,null,1,null,null,1,null,null,1,1,null,null,null,1,1,1,1,1,1,null,null,1,null,null,null,null,1,null,null,1,1,null,null,1,null,null,1,null,null,null,1,null,1,1,null,null,null,1,null,null,null,null,null,1,null,null,1,null,null,1,null,1,1,1,null,1,1,null,null,null,null,1,1,null,null,1,1,1,1,null,null,1,null,1,null,null,1,null,1,null,null,1,1,1,null,1],[null,1,null,null,1,null,1,null,null,null,null,1,null,1,null,null,null,null,null,null,1,1,1,1,1,1,null,null,null,null,null,null,null,1,null,null,null,1,null,null,1,1,null,null,1,null,1,1,null,null,null,null,null,null,null,null,1,1,null,null,1,null,null,1,null,1,1,1,null,1,null,1,null,null,null,null,1,null,null,null,1,1,1,null,null,1,null,1,null,null,1,1,1,null,null,1,1,1,1,1]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>source<\/th>\n      <th>true_target<\/th>\n      <th>pred_wo_attn<\/th>\n      <th>pred_w_attn<\/th>\n      <th>good_wo_attn<\/th>\n      <th>good_w_attn<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":[5,6]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>In aggregate, the predictions from the no-attention model are deemed good 45% of the time and those from the attention model are good 41%. The difference is not statistically significant but it at least shows that the added attention layer did not add any significant improvement to the translation quality.</p>
<p>While reviewing these translations myself, I have also noticed the notorious “<a href="https://arxiv.org/abs/1809.02156">hallucination</a>” problem that plagues the image caption models where, in my case, the model generates pieces of translations that are not present in the source. For example, in the 6th example above, the source sentence is “the wedding will be held in a 17th century church,” but neither predictions get the location of the wedding correct (the first one says on the beach and the second says in a cooking club).<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> The phenomenon is likely due to bias in the language priors (the decoder is a language model after all), which makes me a bit worried about whether those pre-trained language models, despite having been shown to significantly improve many downstream tasks, can potentially exacerbate this particular problem.</p>
<p>Comparisons aside, I was also curious to see whether the learned attention weights could show that the decoder was at least paying attention to the “right” places from the encoder outputs. To get an idea visually, I randomly sampled 9 translations that match the true target exactly and visualized their attention weights below. (The horizontal axis is the source and the vertical is the target. The whiter a cell is, the heavier its attention weight is.) After seeing the results, I can only say… well, kind of.</p>
<p><img src="https://raw.githubusercontent.com/Runze/seq2seq-translation-attention/master/illustrations/attn_weights.png" alt="alt text" width="800"></p>
<p>First of all, as shown above, unlike the crisp, precise patterns shown in the original paper, my attention weights are often spreaded out horizontally, meaning the decoder did not have a good idea where to look at exactly. There are some accurate cases (e.g., in the second matrix, <em>garçon</em> is mapped perfectly to <em>boy</em>, and in the 4th one, <em>pense</em> is mapped to <em>think</em>), but they are generally rather rare. Secondly, the attention weights usually fade out as the decoding timestep increases (i.e., as we move up vertically in the matrix). This is worrisome because it implies that the translations at the later timesteps received little help from the attention layer, which is ironically what the layer is for in the first place. Hence, all things considered, despite increasing the model size and making the training process longer, the attention layer in this case was still under-trained and did not offer too much help in the end.😔</p>
</div>
<div id="conclusion" class="section level3">
<h3>Conclusion</h3>
<p>In this project, I tried to estimate the effect of the attention layer in a seq2seq translation model by performing a simple controlled experiment. After having compared the BLEU scores, manually reviewed a sample of the results, and visualized the learned attention weights, I unfortunately did not find any evidence suggesting any added benefit from the extra attention layer.</p>
<p>That said, as mentioned in the beginning of this post, a <em>big</em> caveat of this study is that the above results are based on this particular dataset and these particular model architectures only. For example, the way I implemented the attention layer is only one of the many ways of doing so and there are <em>many</em> other approaches out there ranging from simple dot products between the previous hidden state and each of the encoder outputs to this rather peculiar way of only using the previous hidden state and the current decoder input as implemented in this PyTorch <a href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#attention-decoder">tutorial</a>.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> Hence, without experimenting with any alternative architectures (or tuning the various hyperparameters), the goal of this project is not at all to say an attention layer is not necessary. Rather, my goal is simply to urge practitioners to always build a <em>solid</em>, albeit boring, baseline first before venturing into a much more complicated solution. What you find may very well surprise you, as it did here for me.🙂</p>
</div>
<div id="references" class="section level3">
<h3>References</h3>
<p>Bahdanau, D., Cho, K., &amp; Bengio, Y. (2015). Neural Machine Translation by Jointly Learning to Align and Translate. In ICLR 2015.</p>
<p>Dichao Hu (2018). An Introductory Survey on Attention Mechanisms in NLP Problems.</p>
<p>Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, Kate Saenko (2018). Object Hallucination in Image Captioning.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>When constructing the references for each predicted translation, I took advantage of the fact that the dataset has multiple target sentence for a single source sentence, so each prediction is evaluated against all the available references.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Both translations also just don’t make sense.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>This survey <a href="https://arxiv.org/abs/1811.05544v1">paper</a> is a good starting point to learn about the different common implementations of the attention layers.<a href="#fnref3">↩</a></p></li>
</ol>
</div>
